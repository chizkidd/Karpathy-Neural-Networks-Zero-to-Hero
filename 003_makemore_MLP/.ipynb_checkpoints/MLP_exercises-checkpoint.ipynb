{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e556cca0",
   "metadata": {},
   "source": [
    "# makemore_MLP\n",
    "\n",
    "----\n",
    "* Inspired by Andrej Karpathy's [\"Building makemore Part 2: MLP\"](https://www.youtube.com/watch?v=TCH_1BHY58I)\n",
    "\n",
    "* Useful links for practice\n",
    "    - [PyTorch internals reference](http://blog.ezyang.com/2019/05/pytorch-internals/)\n",
    "    ```\n",
    "    \"...The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library ... The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch...\" - Edward Z. Yang\n",
    "    ```\n",
    "\n",
    "\n",
    "-----\n",
    "<br><br><a id=\"e1\"></a>\n",
    "# Exercises\n",
    "----\n",
    "1. Tune the hyperparameters of the training to beat [**karpathy's best validation loss of 2.2**.](#1)\n",
    "2. I was not careful with the intialization of the network in this video.\n",
    "    1. What is the loss you'd get if the [predicted probabilities at initialization were perfectly uniform?](#2a) What loss do we achieve? \n",
    "    2. Can you [tune the initialization](#2b) to get a starting loss that is much more similar to (1)?\n",
    "3. Read the **\"A Neural Probabilistic Language Model\"** [paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) (Bengio et al. 2003), [implement and try any idea](#3) from the paper. Did it work?\n",
    "\n",
    "<br><br>![image](_imgs/bengio_et_al_2003_NN_MLP_architecture_.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a248a8",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='1'></a>\n",
    "# 1. Tune Hyperparameters (Val. loss < 2.2)\n",
    "-------\n",
    "The goal is to find an \"optimal\" set of hyperparameters that yield a validation loss that is less than 2.2 (`val_loss < 2.2`). We'll achieve this by iterating over all combinations of two parameters: \n",
    "* `learn_rate`\n",
    "* `batch_size`\n",
    "\n",
    "and selecting the parameter combination with the **lowest validation loss**. Other hyperparameters that can be considered for optimization but weren't in this solution include: \n",
    "* `block_size`\n",
    "* `emb_size`\n",
    "* `hidden_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8a3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1150f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83604128",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:idx + 1 for idx, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {idx: s for s, idx in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe802c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa714fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('batch_size', 16), ('learn_rate', 0.1)): [16, 0.1],\n",
       " (('batch_size', 16), ('learn_rate', 0.05)): [16, 0.05],\n",
       " (('batch_size', 16), ('learn_rate', 0.01)): [16, 0.01],\n",
       " (('batch_size', 32), ('learn_rate', 0.1)): [32, 0.1],\n",
       " (('batch_size', 32), ('learn_rate', 0.05)): [32, 0.05],\n",
       " (('batch_size', 32), ('learn_rate', 0.01)): [32, 0.01],\n",
       " (('batch_size', 64), ('learn_rate', 0.1)): [64, 0.1],\n",
       " (('batch_size', 64), ('learn_rate', 0.05)): [64, 0.05],\n",
       " (('batch_size', 64), ('learn_rate', 0.01)): [64, 0.01],\n",
       " (('batch_size', 128), ('learn_rate', 0.1)): [128, 0.1],\n",
       " (('batch_size', 128), ('learn_rate', 0.05)): [128, 0.05],\n",
       " (('batch_size', 128), ('learn_rate', 0.01)): [128, 0.01],\n",
       " (('batch_size', 256), ('learn_rate', 0.1)): [256, 0.1],\n",
       " (('batch_size', 256), ('learn_rate', 0.05)): [256, 0.05],\n",
       " (('batch_size', 256), ('learn_rate', 0.01)): [256, 0.01]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "block_size = 3   # context length: how many characters do we take to predict the next one?\n",
    "emb_size = 10\n",
    "in_size = block_size * emb_size\n",
    "hidden_size = 200\n",
    "out_size = 27\n",
    "\n",
    "n_iters = 50000\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": [16, 32, 64, 128, 256],\n",
    "    \"learn_rate\": [0.1, 0.05, 0.01]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "result_dict = {}\n",
    "for p in param_grid:\n",
    "    #print(p)\n",
    "    result_dict[tuple(p.items())] = [p[\"batch_size\"], p[\"learn_rate\"]]\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5e9096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92f65ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:18<04:14, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 16, 'learn_rate': 0.1}, training loss: 2.5563, validation loss: 2.2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [00:35<03:52, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 16, 'learn_rate': 0.05}, training loss: 1.9905, validation loss: 2.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [00:52<03:32, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 16, 'learn_rate': 0.01}, training loss: 2.6036, validation loss: 2.5070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [01:13<03:24, 18.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 32, 'learn_rate': 0.1}, training loss: 2.2959, validation loss: 2.2351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 5/15 [01:35<03:16, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 32, 'learn_rate': 0.05}, training loss: 2.1301, validation loss: 2.2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 6/15 [01:56<03:00, 20.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 32, 'learn_rate': 0.01}, training loss: 2.6998, validation loss: 2.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 7/15 [02:20<02:50, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 64, 'learn_rate': 0.1}, training loss: 2.0842, validation loss: 2.2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 8/15 [02:45<02:35, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 64, 'learn_rate': 0.05}, training loss: 2.3601, validation loss: 2.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 9/15 [03:09<02:17, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 64, 'learn_rate': 0.01}, training loss: 2.6763, validation loss: 2.5050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 10/15 [03:40<02:06, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 128, 'learn_rate': 0.1}, training loss: 2.2146, validation loss: 2.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 11/15 [04:11<01:47, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 128, 'learn_rate': 0.05}, training loss: 2.1858, validation loss: 2.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 12/15 [04:42<01:24, 28.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 128, 'learn_rate': 0.01}, training loss: 2.5317, validation loss: 2.5254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 13/15 [05:25<01:05, 32.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 256, 'learn_rate': 0.1}, training loss: 2.2779, validation loss: 2.2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 14/15 [06:09<00:35, 35.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 256, 'learn_rate': 0.05}, training loss: 2.2796, validation loss: 2.2738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:56<00:00, 27.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'batch_size': 256, 'learn_rate': 0.01}, training loss: 2.6329, validation loss: 2.5172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param_dict = {}\n",
    "for pset in tqdm(param_grid):    \n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn((out_size, emb_size), generator = g)\n",
    "    W1 = torch.randn((in_size, hidden_size), generator = g)\n",
    "    b1 = torch.randn(hidden_size, generator = g)\n",
    "    W2 = torch.randn((hidden_size, out_size), generator = g)\n",
    "    b2 = torch.randn(out_size, generator = g)\n",
    "\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "    # print(f\"number of params = {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (pset[\"batch_size\"], ))\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xtr[ix]]                               \n",
    "        h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)     \n",
    "        logits = h @ W2 + b2                           \n",
    "        loss = F.cross_entropy(logits, Ytr[ix])\n",
    "        #print(loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        lr = pset[\"learn_rate\"] if i < n_iters/2 else pset[\"learn_rate\"]/10\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "        # store training loss\n",
    "    #param_dict[tuple(pset.items())] = np.round(loss.item(), decimals=4) \n",
    "    #print(f\"parameters: {pset}, training loss: {loss.item():.4f}, validation loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb = C[Xdev]\n",
    "        h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        val_loss = F.cross_entropy(logits, Ydev)\n",
    "        \n",
    "    param_dict[tuple(pset.items())] = [np.round(loss.item(), decimals=4), np.round(val_loss.item(), decimals=4)]\n",
    "    print(f\"parameters: {pset}, training loss: {loss.item():.4f}, validation loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581dfd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Extract the parameters and validation losses from param_dict\n",
    "params = list(param_dict.keys())\n",
    "val_losses = [param_dict[p][1] for p in params]\n",
    "\n",
    "# Create a 2D array to store the validation losses\n",
    "val_loss_array = np.array(val_losses).reshape(len(set(p[0][1] for p in params)), len(set(p[1][1] for p in params)))\n",
    "\n",
    "# Get the unique batch sizes and learning rates\n",
    "batch_sizes = sorted(set(p[0][1] for p in params))\n",
    "learn_rates = sorted(set(p[1][1] for p in params))\n",
    "\n",
    "# Create a heatmap\n",
    "plt.imshow(val_loss_array, cmap='viridis', interpolation='nearest')\n",
    "#plt.imshow(val_loss_array, cmap='gray_r', interpolation='nearest')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Batch Size')\n",
    "plt.title('Validation Loss')\n",
    "plt.colorbar()\n",
    "\n",
    "# Set the tick labels and values\n",
    "plt.xticks(range(len(learn_rates)), learn_rates)\n",
    "plt.yticks(range(len(batch_sizes)), batch_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9749e639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('batch_size', 16), ('learn_rate', 0.1)): [2.5563, 2.257],\n",
       " (('batch_size', 16), ('learn_rate', 0.05)): [1.9905, 2.3013],\n",
       " (('batch_size', 16), ('learn_rate', 0.01)): [2.6036, 2.507],\n",
       " (('batch_size', 32), ('learn_rate', 0.1)): [2.2959, 2.2351],\n",
       " (('batch_size', 32), ('learn_rate', 0.05)): [2.1301, 2.286],\n",
       " (('batch_size', 32), ('learn_rate', 0.01)): [2.6998, 2.5056],\n",
       " (('batch_size', 64), ('learn_rate', 0.1)): [2.0842, 2.2304],\n",
       " (('batch_size', 64), ('learn_rate', 0.05)): [2.3601, 2.2766],\n",
       " (('batch_size', 64), ('learn_rate', 0.01)): [2.6763, 2.505],\n",
       " (('batch_size', 128), ('learn_rate', 0.1)): [2.2146, 2.2326],\n",
       " (('batch_size', 128), ('learn_rate', 0.05)): [2.1858, 2.2696],\n",
       " (('batch_size', 128), ('learn_rate', 0.01)): [2.5317, 2.5254],\n",
       " (('batch_size', 256), ('learn_rate', 0.1)): [2.2779, 2.2428],\n",
       " (('batch_size', 256), ('learn_rate', 0.05)): [2.2796, 2.2738],\n",
       " (('batch_size', 256), ('learn_rate', 0.01)): [2.6329, 2.5172]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c88078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest validation loss value is 2.2304 and the parameters are (('batch_size', 64), ('learn_rate', 0.1))\n"
     ]
    }
   ],
   "source": [
    "min_key = min(param_dict, key=lambda k: param_dict[k][1]) # retrieve 2nd element in dict value (val loss)\n",
    "min_value = param_dict[min_key]\n",
    "print(f\"The smallest validation loss value is {min_value[1]} and the parameters are {min_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523f9103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 0.1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_key[0][1], min_key[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca171914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params = 11897\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((out_size, emb_size), generator = g)\n",
    "W1 = torch.randn((in_size, hidden_size), generator = g)\n",
    "b1 = torch.randn(hidden_size, generator = g)\n",
    "W2 = torch.randn((hidden_size, out_size), generator = g)\n",
    "b2 = torch.randn(out_size, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"number of params = {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b9228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "batch_size = 256\n",
    "n_iters = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f6d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch 0 | loss 26.6556 at lr 0.1\n",
      "mini batch 10000 | loss 2.3171 at lr 0.1\n",
      "mini batch 20000 | loss 2.2889 at lr 0.1\n",
      "mini batch 30000 | loss 2.2101 at lr 0.1\n",
      "mini batch 40000 | loss 2.2581 at lr 0.1\n",
      "mini batch 50000 | loss 2.2627 at lr 0.1\n",
      "mini batch 60000 | loss 2.3664 at lr 0.1\n",
      "mini batch 70000 | loss 2.0849 at lr 0.1\n",
      "mini batch 80000 | loss 2.3078 at lr 0.1\n",
      "mini batch 90000 | loss 2.1750 at lr 0.1\n",
      "mini batch 100000 | loss 2.1651 at lr 0.1\n",
      "mini batch 110000 | loss 2.0781 at lr 0.01\n",
      "mini batch 120000 | loss 2.1338 at lr 0.01\n",
      "mini batch 130000 | loss 2.1463 at lr 0.01\n",
      "mini batch 140000 | loss 2.1469 at lr 0.01\n",
      "mini batch 150000 | loss 2.0903 at lr 0.001\n",
      "mini batch 160000 | loss 2.1117 at lr 0.001\n",
      "mini batch 170000 | loss 2.1610 at lr 0.001\n",
      "mini batch 180000 | loss 2.1123 at lr 0.001\n",
      "mini batch 190000 | loss 2.1595 at lr 0.001\n",
      "val_loss: 2.160660982131958\n",
      "CPU times: user 3min 29s, sys: 23.3 s, total: 3min 53s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Essential to always enable gradients !!!\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Training loop !!!\n",
    "for i in range(n_iters):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]                               \n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)     \n",
    "    logits = h @ W2 + b2                           \n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # add regularization\n",
    "    #loss += 0.01 * (W1 ** 2).mean() + 0.01 * (W2 ** 2).mean()\n",
    "    #print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    if i > 0.5 * n_iters and i < 0.75 * n_iters:\n",
    "        lr = learn_rate / 10\n",
    "    elif i >= 0.75 * n_iters:\n",
    "        lr = learn_rate / 100\n",
    "    else:\n",
    "        lr = learn_rate\n",
    "#     r = 10E-5 # n_iters = 300000 & lr_init = 0.1\n",
    "#     lr = 0.1 / (1 + r*i)   \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f\"mini batch {i} | loss {loss.item():.4f} at lr {lr}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    emb = C[Xdev]\n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Ydev)\n",
    "    print(f\"val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fde02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss: 2.1297\n",
      "The validation loss: 2.1607\n"
     ]
    }
   ],
   "source": [
    "# Evaluation - Training Loss\n",
    "\n",
    "emb_tr = C[Xtr]                               \n",
    "h_tr  = torch.tanh(emb_tr.view(-1, in_size) @ W1 + b1)   \n",
    "logits_tr = h_tr @ W2 + b2                        \n",
    "loss_tr = F.cross_entropy(logits_tr, Ytr)\n",
    "print(f\"The training loss: {loss_tr.item():.4f}\")\n",
    "\n",
    "# Evaluation - Validation Loss\n",
    "emb_dev = C[Xdev]                                \n",
    "h_dev = torch.tanh(emb_dev.view(-1, in_size) @ W1 + b1)    \n",
    "logits_dev = h_dev @ W2 + b2                        \n",
    "loss_dev = F.cross_entropy(logits_dev, Ydev)\n",
    "print(f\"The validation loss: {loss_dev.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10b07b",
   "metadata": {},
   "source": [
    "**Comment:** The validation loss of the model based on `batch_size = 256` and `learn_rate = 0.01` is **2.152** which beats <u>2.2</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6b45b",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='2a'></a>\n",
    "# 2a. Initialization: Perfectly Uniform Predicted Probabilities\n",
    "-------\n",
    "We have 27 possible characters (26 letters of the alphabet and 1 special token, `.`). If every character was equally likely, then we'd expect all the probabilities to be roughly 4% (`1/27.0 ~= 3.7%`), in essence the predicted probabilities at initialization are perfectly uniform.\n",
    "\n",
    "$$\\\\{logits}_{ij} = \\frac{e^{C_{ij}}}{\\sum_{j} e^{C_{ij}}} = \\frac{C}{27C} = \\frac{1}{27}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1d63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "emb_size = 10\n",
    "in_size = block_size * emb_size\n",
    "hidden_size = 200\n",
    "out_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66a6104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.full((out_size, emb_size), fill_value=1.0)\n",
    "W1 = torch.full((in_size, hidden_size), fill_value=1.0)\n",
    "b1 = torch.full((1,hidden_size), fill_value=1.0)\n",
    "W2 = torch.full((hidden_size, out_size), fill_value=1.0)\n",
    "b2 = torch.full((1,out_size), fill_value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a95464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        ...,\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370],\n",
      "        [0.0370, 0.0370, 0.0370,  ..., 0.0370, 0.0370, 0.0370]])\n",
      "The training loss: 3.2958\n",
      "\n",
      "The validation loss: 3.2958\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "print(torch.softmax(logits, dim=1))\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "\n",
    "print(f\"The training loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "emb_dev = C[Xdev]                                \n",
    "h_dev = torch.tanh(emb_dev.view(-1, in_size) @ W1 + b1)    \n",
    "logits_dev = h_dev @ W2 + b2                        \n",
    "loss_dev = F.cross_entropy(logits_dev, Ydev)\n",
    "print(f\"The validation loss: {loss_dev.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064aa35",
   "metadata": {},
   "source": [
    "**Comment:** The loss, if the predicted probabilities at initialization were perfectly uniform, is **3.296**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abb0e3",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='2b'></a>\n",
    "# 2b. Tune Initialization (Similar Val. loss to that of No. 1)\n",
    "-----\n",
    "\n",
    "We initialize the weights and biases to be as close to zero as possible. I calibrated the variances with `sqrt (2/n)` based on a stanford cs231n lecture on [weight initialization.](https://cs231n.github.io/neural-networks-2/#init)\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ea7f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00790162092698779"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(2/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b1a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params = 11897\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((out_size, emb_size), generator = g) \n",
    "W1 = torch.randn((in_size, hidden_size), generator = g) * np.sqrt(2/len(words))\n",
    "b1 = torch.randn(hidden_size, generator = g) * 0.0\n",
    "W2 = torch.randn((hidden_size, out_size), generator = g) * np.sqrt(2/len(words))\n",
    "b2 = torch.randn(out_size, generator = g) * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"number of params = {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ed6426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch 0 | loss 3.2959 at lr 0.1\n",
      "mini batch 10000 | loss 2.2430 at lr 0.1\n",
      "mini batch 20000 | loss 2.2546 at lr 0.1\n",
      "mini batch 30000 | loss 2.1073 at lr 0.01\n",
      "mini batch 40000 | loss 2.2433 at lr 0.01\n",
      "\n",
      "val_loss: 2.1384\n",
      "\n",
      "CPU times: user 54.1 s, sys: 6.18 s, total: 1min\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_iters = 50000\n",
    "# Essential to always enable gradients !!!\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Training loop !!!\n",
    "for i in range(n_iters):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]                               \n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)     \n",
    "    logits = h @ W2 + b2                           \n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # add regularization\n",
    "    #loss += 0.01 * (W1 ** 2).mean() + 0.01 * (W2 ** 2).mean()\n",
    "    #print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    if i > 0.5 * n_iters:\n",
    "        lr = learn_rate / 10\n",
    "    elif i >= 0.75 * n_iters:\n",
    "        lr = learn_rate / 100\n",
    "    else:\n",
    "        lr = learn_rate\n",
    "#     r = 10E-5 # n_iters = 300000 & lr_init = 0.1\n",
    "#     lr = 0.1 / (1 + r*i)   \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f\"mini batch {i} | loss {loss.item():.4f} at lr {lr}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    emb = C[Xdev]\n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Ydev)\n",
    "    print(f\"\\nval_loss: {val_loss.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aaa9e5",
   "metadata": {},
   "source": [
    "**Comment:** After tuning the initialization, the validation loss after training is **2.138** which is similar to the validation loss in no. 1 of **2.152**. The starting loss here of **3.296** is better than that of no. 1 which is **25.74**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1987e",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='3'></a>\n",
    "# 3. Implement New Idea from Bengio 2003 Neural Probabilistic LM Paper\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06e219",
   "metadata": {},
   "source": [
    "Let's **add an additional layer, the word features input to output weights $W$ (a $|V| \\times(n-1) m$ matrix).** We'll denote it as $W_{0}$.\n",
    "\n",
    "    \n",
    "$$\n",
    "\\begin{equation*}\n",
    "y=b_{2}+W_{0} x+W_{2} \\tanh (b_{1}+W_{1} x) \\tag{1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "parameters: \\theta=(b_{1}, b_{2}, W_{0}, W_{1}, W_{2}, C)\n",
    "$$<br>\n",
    "Also, we'll add a **ridge regularization penalty and only apply it to the weights ($W_{0}, W_{1}, W_{2}$) and not the biases** as per the paper. Finally, we'll implement [**SGD with momentum**](https://paperswithcode.com/method/sgd-with-momentum) since our gradient trajectory is noisy during our mini-batch gradient descent training loop, and this limits our use of large learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "235e6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "emb_size = 10\n",
    "in_size = block_size * emb_size\n",
    "hidden_size = 200\n",
    "out_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9db8aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params = 12707\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((out_size, emb_size), generator = g)\n",
    "W1 = torch.randn((in_size, hidden_size), generator = g)\n",
    "b1 = torch.randn(hidden_size, generator = g)\n",
    "W2 = torch.randn((hidden_size, out_size), generator = g)\n",
    "b2 = torch.randn(out_size, generator = g)\n",
    "W0 = torch.randn((in_size, out_size), generator=g)\n",
    "parameters = [C, W1, b1, W2, b2, W0]\n",
    "param_grads = [0.0 for _ in range(len(parameters))]\n",
    "print(f\"number of params = {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e649e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "batch_size = 256\n",
    "n_iters = 200000\n",
    "beta = 0.9 # average over last 10 values.\n",
    "reg = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6982ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch 0 | loss 29.5452 at lr 0.1\n",
      "mini batch 10000 | loss 2.2582 at lr 0.1\n",
      "mini batch 20000 | loss 2.2082 at lr 0.1\n",
      "mini batch 30000 | loss 2.3550 at lr 0.1\n",
      "mini batch 40000 | loss 2.4241 at lr 0.1\n",
      "mini batch 50000 | loss 2.2602 at lr 0.1\n",
      "mini batch 60000 | loss 2.1515 at lr 0.1\n",
      "mini batch 70000 | loss 2.2701 at lr 0.1\n",
      "mini batch 80000 | loss 2.2038 at lr 0.1\n",
      "mini batch 90000 | loss 2.1660 at lr 0.1\n",
      "mini batch 100000 | loss 2.1656 at lr 0.1\n",
      "mini batch 110000 | loss 2.2317 at lr 0.01\n",
      "mini batch 120000 | loss 2.2038 at lr 0.01\n",
      "mini batch 130000 | loss 2.3513 at lr 0.01\n",
      "mini batch 140000 | loss 2.1198 at lr 0.01\n",
      "mini batch 150000 | loss 2.1437 at lr 0.001\n",
      "mini batch 160000 | loss 2.2227 at lr 0.001\n",
      "mini batch 170000 | loss 2.2649 at lr 0.001\n",
      "mini batch 180000 | loss 2.0970 at lr 0.001\n",
      "mini batch 190000 | loss 2.0675 at lr 0.001\n",
      "val_loss: 2.155996799468994\n",
      "CPU times: user 4min 53s, sys: 23.7 s, total: 5min 17s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Essential to always enable gradients !!!\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Training loop !!!\n",
    "for i in range(n_iters):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]                               \n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)     \n",
    "    logits = emb.view(-1, in_size) @ W0 + h @ W2 + b2                           \n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # add regularization\n",
    "    loss += reg * ( (W0 ** 2).mean() + (W1 ** 2).mean() +  (W2 ** 2).mean())\n",
    "    #print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    if i > 0.5 * n_iters and i < 0.75 * n_iters:\n",
    "        lr = learn_rate / 10\n",
    "    elif i >= 0.75 * n_iters:\n",
    "        lr = learn_rate / 100\n",
    "    else:\n",
    "        lr = learn_rate\n",
    "        \n",
    "    for idx, p in enumerate(parameters):\n",
    "        param_grads[idx] = beta * param_grads[idx] + (1 - beta) * p.grad\n",
    "        p.data = p.data - lr * param_grads[idx]\n",
    "\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        print(f\"mini batch {i} | loss {loss.item():.4f} at lr {lr}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    emb = C[Xdev]\n",
    "    h = torch.tanh(emb.view(-1, in_size) @ W1 + b1)\n",
    "    logits = emb.view(-1, in_size) @ W0 + h @ W2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Ydev)\n",
    "    print(f\"val_loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d67f0e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss: 2.1310\n",
      "The validation loss: 2.1560\n"
     ]
    }
   ],
   "source": [
    "# Evaluation - Training Loss\n",
    "\n",
    "emb_tr = C[Xtr]                               \n",
    "h_tr  = torch.tanh(emb_tr.view(-1, in_size) @ W1 + b1)   \n",
    "logits_tr = emb_tr.view(-1, in_size) @ W0 + h_tr @ W2 + b2                      \n",
    "loss_tr = F.cross_entropy(logits_tr, Ytr)\n",
    "print(f\"The training loss: {loss_tr.item():.4f}\")\n",
    "\n",
    "# Evaluation - Validation Loss\n",
    "emb_dev = C[Xdev]                                \n",
    "h_dev = torch.tanh(emb_dev.view(-1, in_size) @ W1 + b1)    \n",
    "logits_dev = emb_dev.view(-1, in_size) @ W0 + h_dev @ W2 + b2                       \n",
    "loss_dev = F.cross_entropy(logits_dev, Ydev)\n",
    "print(f\"The validation loss: {loss_dev.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd51ca",
   "metadata": {},
   "source": [
    "**Comment:** The validation loss is **2.156** with the <u>addition of a new layer $W_{0}$, ridge regularization penalty, and stochastic gradient descent momentum.</u> It is roughly the same as the validation loss of **2.1524** in no. 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c06cd1",
   "metadata": {},
   "source": [
    "----\n",
    "### Sampling from the model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd093b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "ambrilli.\n",
      "kimri.\n",
      "rehty.\n",
      "skaf.\n",
      "ske.\n",
      "rahnen.\n",
      "den.\n",
      "rha.\n",
      "kaqui.\n",
      "ner.\n",
      "kia.\n",
      "chaiir.\n",
      "kaleigph.\n",
      "bmandin.\n",
      "quint.\n",
      "sulin.\n",
      "alian.\n",
      "quinathon.\n",
      "jarynix.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = emb.view(-1, in_size) @ W0 + h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
