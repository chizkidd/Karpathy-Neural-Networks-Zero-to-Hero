{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore_Backprop\n",
    "\n",
    "----\n",
    "* Inspired by Andrej Karpathy's [\"Building makemore Part 4: Becoming a Backprop Ninja\"](https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6s)\n",
    "\n",
    "* Supplementary links \n",
    "    - 'Yes you should understand backprop' - Karpathy, Dec 2016 [blog link](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "    - [BatchNorm paper](https://arxiv.org/abs/1502.03167)\n",
    "    - [Bessel's Correction](https://math.oxford.emory.edu/site/math117/besselCorrection/)\n",
    "    - Bengio et al. 2003 MLP language model paper [(pdf)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "------------------\n",
    "- [0. Makemore: Introduction](#0)\n",
    "- [1. Starter Code](#1)\n",
    "- [2. Exercise 1: Backpropagation through the Atomic Compute Graph](#2)\n",
    "- [3. Bessel's Correction in `BatchNorm`](#3)\n",
    "- [4. Exercise 2: Cross Entropy Loss Backward Pass](#4)\n",
    "- [5. Exercise 3: `BatchNorm` Layer Backward Pass](#5)\n",
    "- [6. Exercise 4: Putting it all Together](#6)\n",
    "- [7. Conclusion](#7)\n",
    "------\n",
    "\n",
    "\n",
    "\n",
    "# Appendix\n",
    "---------------\n",
    "- [A1. Linear Layer Manual Backpropagation: Simpler Case](#a1)\n",
    "- [A2. Broadcasting Examples: Simpler Cases](#a2)\n",
    "\n",
    "## [References](#r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br>\n",
    "# 0. Makemore: Introduction<a id=\"0\"></a>\n",
    "---------------------------------\n",
    "<u>**Makemore**</u> takes one text file as input, where each line is assumed to be one training thing, and generates more things like it. Under the hood, it is an **autoregressive character-level language model**, with a <u>wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT)</u>. An [autoregressive model](https://en.wikipedia.org/wiki/Autoregressive_model) specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term). For example, we can feed it a database of names, and makemore will generate cool baby name ideas that all sound name-like, but are not already existing names. Or if we feed it a database of company names then we can generate new ideas for a name of a company. Or we can just feed it valid scrabble words and generate english-like babble. \n",
    "```\n",
    "\"As the name suggests, makemore makes more.\"\n",
    "```\n",
    "This is not meant to be too heavyweight of a library with a billion switches and knobs. It is one hackable file, and is mostly intended for educational purposes. [PyTorch](https://pytorch.org) is the only requirement.\n",
    "\n",
    "Current implementation follows a few key papers:\n",
    "\n",
    "- Bigram (one character predicts the next one with a lookup table of counts)\n",
    "- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)\n",
    "- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "- LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "- GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "\n",
    "In the 3rd makemore tutorial notebook, we covered key concepts for training neural networks, including forward pass activations, backward pass gradients, and batch normalization, with a focus on implementing and optimizing MLPs and ResNets in `PyTorch`. We also introduced diagnostic tools to monitor neural network health and highlights important considerations like proper weight initialization and learning rate selection.\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we take the **2-layer MLP (with BatchNorm)** from the previous video and **backpropagate through it <u>manually</u> without using `PyTorch` autograd's `loss.backward()` through the cross entropy loss, 2nd `Linear` layer, `tanh`, `batchnorm`, 1st `Linear` layer, and the embedding table.** Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in **micrograd.** This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"1\"></a>\n",
    "# 1. Starter Code\n",
    "-----\n",
    "Let's recap the MLP model we implemented in **part 2 & 3 (MLP) of the makemore series**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "moY8IOKlrm9b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KZE_EtNru_P",
    "outputId": "c3e110f0-0be1-45de-d32f-4b0bec54bb97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../data/names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXRALF1xr2od",
    "outputId": "93151948-a590-4a18-c21a-f0bcb96f83e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lY9RYt-or3pu",
    "outputId": "c1d93524-6718-49f6-aca5-98a0fa053545",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "`cmp` will be used to compare and check manual gradients to `PyTorch` gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We'll implement a significantly expanded version of forward pass because:\n",
    "* full explicit implementation of loss function instead of using `F.cross_entropy`\n",
    "* break-up of implementation into manageable chunks\n",
    "    - allows for step-by-step backward calculation of gradients from bottom to top (upwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3514, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability (so exponential does not explode)\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"2\"></a>\n",
    "# 2.  Exercise 1: Backpropagation through the Atomic Compute Graph\n",
    "-----\n",
    "The goal is to manually calculate the loss gradient $w.r.t$ to all the preceding parameters step-by-step. Let's go through each parameter. [Note: `di = dloss/di` for all network parameters]\n",
    "* `dlogprobs` = `torch.zeros_like(logprobs)` $\\rightarrow$ `dlogprobs[range(n), Yb]` = `-1.0/n`\n",
    "    - to calculate`loss`, we choose only the target characters from the `logprobs` matrix. Thus, only those positions (target positions) will be affected by the backprop. Since we average across the mini-batches, we must divide by the mini-batch size (`n` = $32$).\n",
    "    - simpler example: using `loss = -(a+b+c)/3` where `n` = $3$, `dloss/da = dloss/db = dloss/dc = -1/3`\n",
    "    - see [Appendix A2](#a2) for broadcasting guidance.\n",
    "<br><br>   \n",
    "* `dprobs` = `(1/probs) * dlogprobs` \n",
    "    - this basically either passes the `dlogprobs` if the `prob` is significantly close to $1$ (correct predictions) or boosts the gradients of incorrectly-assigned probabilities (correct character with a low `prob` = incorrect predictions).\n",
    "    - using $\\boldsymbol{\\frac{dln(x)}{dx} = \\frac{1}{x}}$  and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{dx} = \\frac{dln(x)}{dx} \\times \\frac{dy}{dln(x)}}$\n",
    "<br><br>    \n",
    "* `dcounts_sum_inv` = `(counts * dprobs).sum(1, keepdim = True)`\n",
    "    - `counts_sum_inv` is a $[32, 1]$ matrix and `counts` is a $[32, 27]$ matrix. There is a **broadcasting** happening under the hood. Let's look at a simple example. <br><img src=\"_imgs/dcounts_sum_inv_broadcasting.png\" alt=\"broadcasting\" width=\"500\" ><br> Since $b1$ appears in two places of the resulting matrix, we must accumulate the gradients. Thus, we must sum the gradient matrix across the rows (`.sum(1, keepdim = True)`).\n",
    "    - Remember: **Broadcasting in the forward pass creates a summation in the backward pass.** See [Appendix A2.](#a2)\n",
    "    - using $\\boldsymbol{f = a\\times b, \\frac{df}{da} = b}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "  <br><br>  \n",
    "* `dcounts_sum` = `(-counts_sum**-2) * dcounts_sum_inv`\n",
    "    - using $\\boldsymbol{\\frac{dx^n}{dx} = nx^{(n-1)}}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{dx} = \\frac{dx^n}{dx} \\times \\frac{dy}{dx^n}}$\n",
    "<br><br>    \n",
    "* `dcounts` = `dprobs * counts_sum_inv`$\\rightarrow$ `dcounts += torch.ones_like(counts)* dcounts_sum`\n",
    "    - using  $\\boldsymbol{f = a\\times b, \\frac{df}{da} = b}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "    - `count` contributes to the derivative in two paths shown below. Thus, we must add both derivatives. In `counts_sum`, we sum `count` across the rows. **Summation in the forward pass means broadcasting in the backward pass.** See [Appendix A2.](#a2)<br>\n",
    "        ```\n",
    "        >>>probs = counts * counts_sum_inv\n",
    "        >>>counts_sum = counts.sum(1, keepdims=True)\n",
    "        ```\n",
    "    - The code above shows the relevant section(s) of the forward pass pertaining to `counts`\n",
    "<br><br>\n",
    "* `dnorm_logits` = `norm_logits.exp() * dcounts`\n",
    "    - using $\\boldsymbol{\\frac{de^{x}}{dx} = e^{x}}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{dx} = \\frac{de^x}{dx} * \\frac{dy}{de^x}}$\n",
    "<br><br> \n",
    "* `dlogit_maxes` = `(-1.0 * dnorm_logits).sum(1, keepdim = True)`\n",
    "    - using $\\boldsymbol{f = a - b, \\frac{df}{db} = -1}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{db} = \\frac{df}{db} \\times \\frac{dy}{df}}$\n",
    "    - The shapes of `logits` and `logit_maxes` are $[32, 27]$ and $[32, 1]$ respectively. The `logit_maxes` matrix is **broadcasted across the rows.** Thus, we must accumulate the gradients (sum gradient matrix across the rows). Since, this is a simple addition, the gradient directly flows.\n",
    "    - `logit_maxes` does not affect the overall distribution of the probabilities and is only used for numerical stability. Thus, `dlogit_maxes` must be very close to $0$.\n",
    "<br><br>     \n",
    "* `dlogits` = `1 * dnorm_logits` $\\rightarrow$ `dlogits += F.one_hot(logits.max(1).indices, num_classes = 27) * dlogit_maxes`\n",
    "    - using $\\boldsymbol{f = a - b, \\frac{df}{da} = 1}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "    - Logits affects the forward pass in two ways.<br>\n",
    "        ```\n",
    "        >>>logit_maxes = logits.max(1, keepdim=True).values\n",
    "        >>>norm_logits = logits - logit_maxes\n",
    "        ```\n",
    "    - `.max` function gives us both maximum values and their positions. Only the maximum values will be affected through `logit_maxes`. \n",
    "    - See [Appendix A2.](#a2)\n",
    "<br><br>     \n",
    "* `dh` = `dlogits @ W2.T`, `dW2` = `h.T @ dlogits`, `db2` = `dlogits.sum(0)`\n",
    "    - See [Appendix A1.](#a1)\n",
    "<br><br>    \n",
    "* `dhpreact` = `(1 - h**2) * dh`\n",
    "    - using $\\boldsymbol{f = \\tanh(x), \\frac{df}{dx} = 1 - f^2(x)}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{dx} = \\frac{df}{dx} \\times \\frac{dy}{df}}$\n",
    "<br><br> \n",
    "* `dbngain` = `(bnraw * dhpreact).sum(0, keepdim=True)` $\\rightarrow$ `dbnbias` = `(1 * dhpreact).sum(0, keepdim=True)`\n",
    "    - `bngain` and `bnbias` are both $[1, 64]$ matrices. `bnraw` is a $[32, 64]$ matrix. Both `bngain` and `bnbias` matrices are **broadcasted across the columns.** Thus, we must accumulate the gradients (sum gradient matrix across the columns). \n",
    "    - using $\\boldsymbol{f = (a\\times b) + c, \\frac{df}{da} = b, \\frac{df}{dc} = 1}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$, $\\boldsymbol{\\frac{dy}{dc} = \\frac{df}{dc} \\times \\frac{dy}{df}}$\n",
    "<br><br> \n",
    "* `dbnraw` = `bngain * dhpreact`\n",
    "    - using $\\boldsymbol{f = (a\\times b), \\frac{df}{da} = b}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "<br><br> \n",
    "* `dbnvar_inv` = `(bndiff * dbnraw).sum(0, keepdim=True)`\n",
    "    - `bnvar_inv` is a $[1, 64]$ matrix and `bndiff` is a $[32, 64]$ matrix. So `bnvar_inv` matrix is **broadcasted across the columns.** Thus, we must accumulate the gradients (sum gradient matrix across the columns). \n",
    "    - using $\\boldsymbol{f = (a\\times b), \\frac{df}{da} = b}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "<br><br> \n",
    "* `dbnvar` = `-0.5*(bnvar + 1e-5)**(-1.5) * dbnvar_inv`\n",
    "    - using $\\boldsymbol{\\frac{dx^n}{dx} = nx^{(n-1)}}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{dx} = \\frac{dx^n}{dx} \\times \\frac{dy}{dx^n}}$\n",
    "<br><br> \n",
    "* `dbndiff2` = `(1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar`\n",
    "    - using $\\boldsymbol{f = \\frac{a}{const + 1}, \\frac{df}{da} = \\frac{1}{const + 1}}$ and <u>chain rule</u>: $\\boldsymbol{\\frac{dy}{da} = \\frac{df}{da} \\times \\frac{dy}{df}}$\n",
    "    - `bndiff2` is a $[32, 64]$ matrix and `bnvar` is a $[1, 64]$ matrix. Thus, we must add both derivatives. In `bnvar`, we sum `bndiff2` across the rows. **Summation in the forward pass means broadcasting in the backward pass.** We can use a <u>ones matrix to implement the broadcasting operation.</u><br>\n",
    "        ```\n",
    "        >>>bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "        ```\n",
    "    - The code above shows the relevant section(s) of the forward pass pertaining to `bndiff2`\n",
    "<br><br> \n",
    "* `dbndiff` = `(bnvar_inv * dbnraw) + (2 * bndiff * dbndiff2)`\n",
    "    - `bndiff` contributes to the derivative in two paths shown below, therefore we must add both derivatives. Recall the <u>multivariate chain rule:</u> $\\boldsymbol{ \\frac{df}{dt} = \\frac{df}{da}\\frac{da}{dt} + \\frac{df}{db}\\frac{db}{dt}  }$ <br>\n",
    "        ```\n",
    "        >>>bndiff2 = bndiff**2\n",
    "        >>>bnraw = bndiff * bnvar_inv\n",
    "        ```\n",
    "    - The code above shows the relevant section(s) of the forward pass pertaining to `bndiff`\n",
    "<br><br> \n",
    "* `dbnmeani` = `(-1.0 * dbndiff).sum(0, keepdim=True)`\n",
    "    - see `dbnbias` and `dbnvar_inv` sections for reference\n",
    "<br><br>\n",
    "* `dhprebn` = `1.0 * dbndiff` $\\rightarrow$ `dhprebn += (1.0/n) * torch.ones_like(hprebn) * dbnmeani`\n",
    "    - see `dlogits` and `dbndiff2` sections for reference\n",
    "<br><br>\n",
    "* `dembcat` = `dhprebn @ W1.T`, `dW1` = `embcat.T @ dhprebn`, `db1` = `dhprebn.sum(0)`\n",
    "    - See [Appendix A1.](#a1) \n",
    "<br><br>\n",
    "* `demb` = `dembcat.view(emb.shape)`\n",
    "    - We need to unpack the gradients since we have concatenated the embeddings. We can use the `.view` operation.<br>\n",
    "        ```\n",
    "        >>>embcat = emb.view(emb.shape[0], -1) \n",
    "        ```\n",
    "    - The code above shows the relevant section of the forward pass pertaining to `emb`\n",
    "<br><br> \n",
    "* `dC` \n",
    "    - This gradient was a bit more tricky. We basically have to flow back the embedding gradients to the original embedding space.\n",
    "    - `emb.shape = [32,3,10]` and `C.shape = [27,10]`. \n",
    "    - One way to handle this is by <u>iterating through the rows and columns of `Xb` to map every embedding gradient to the corresponding embedding.</u> \n",
    "    - Also bear in mind, some embedding locations might appear multiple times so we have to sum the gradients in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9346, -3.1014, -3.6580, -3.3507, -4.0988, -3.4880, -3.2290, -4.0731,\n",
       "        -3.2338, -4.2270, -3.1196, -1.7151, -2.7638, -3.0241, -2.9716, -3.2359,\n",
       "        -3.7394, -3.0354, -3.5403, -3.3955, -2.8156, -3.0766, -4.2790, -3.9457,\n",
       "        -3.5833, -2.8992, -3.0532, -3.8914, -2.8200, -3.5050, -3.3221, -3.1180],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb25c0a00b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see where the max values are located\n",
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts.shape, counts_sum_inv.shape, dprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape, logit_maxes.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dlogits.shape, h.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dhpreact.shape, bngain.shape, bnbias.shape, bnraw.shape, (bngain * dhpreact).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#bnraw.shape, bndiff.shape, bnvar_inv.shape, (bndiff*dbnraw).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnvar_inv.shape, bnvar.shape,(-0.5*bnvar_inv**3 * dbnvar_inv).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar.shape, bndiff2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bndiff.shape, (bnvar_inv * dbnraw).shape, (2*bndiff*dbndiff2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bndiff.shape, hprebn.shape, bnmeani.shape, (-1*dbndiff).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bnmeani.shape, hprebn.shape, (torch.ones_like(hprebn)*dbndiff).shape, ((1.0/n) * torch.ones_like(hprebn) * dbnmeani).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hprebn.shape, \\\n",
    "# embcat.shape, (dhprebn @ W1.T).shape, \\\n",
    "# W1.shape, (embcat.T @ dhprebn).shape, \\\n",
    "# b1.shape, dhprebn.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embcat.shape, emb.shape, dembcat.view(emb.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#emb.shape, C.shape, Xb.shape, demb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True)\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts)* dcounts_sum\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "dlogit_maxes = (-1.0 * dnorm_logits).sum(1, keepdim = True)\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]) * dlogit_maxes\n",
    "# dlogits2 = torch.zeros_like(logits)\n",
    "# dlogits2[range(logits.shape[0]), logits.max(1).indices] = 1.0\n",
    "# dlogits += dlogits2 * dlogit_maxes\n",
    "dh = dlogits @ W2.T # [32 27] * [27 64] -> [32 64]\n",
    "dW2 = h.T @ dlogits # [64 32] * [32 27] -> [64 27]\n",
    "db2 = dlogits.sum(0) # [32 27].sum(0) -> [27]\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = (1.0 * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**(-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff = (bnvar_inv * dbnraw) + (2 * bndiff * dbndiff2)\n",
    "dbnmeani = (-1.0 * dbndiff).sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone() #torch.ones_like(hprebn) * dbndiff\n",
    "dhprebn += (1.0/n) * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"3\"></a>\n",
    "# 3.  Bessel's Correction in `BatchNorm`\n",
    "-----\n",
    "Observe that we have used a different formula from the conventional definition of variance in the <u>implementation of BatchNorm.</u> This is called the **Besselâ€™s correction.** When we sample a batch from a distribution and calculate the variance of the batch, we must divide the squared differences by n-1, where n is the batch size. Dividing by n gives us a biased estimation which introduces a train-test mismatch. The train-test mismatch/discrepancy occurs when the biased version is used for training while the unbiased estimation is used during inference (estimating running standard deviation).\n",
    "```\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"4\"></a>\n",
    "# 4.  Exercise 2: Cross Entropy Loss Backward Pass\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3513824939727783 diff: 7.152557373046875e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -=1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0014,  0.0008,  0.0022,  0.0006,  0.0022,  0.0009,  0.0005,  0.0007,\n",
       "         0.0005,  0.0016,  0.0008,  0.0010,  0.0011,  0.0025,  0.0012,  0.0013,\n",
       "         0.0016,  0.0013, -0.0299,  0.0005,  0.0019,  0.0003,  0.0018,  0.0008,\n",
       "         0.0006,  0.0012,  0.0007], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's take a look into `dlogits` to get an intuition of its meaning. It turns out it has a beautiful and quite simple explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0687, 0.0913, 0.0201, 0.0489, 0.0185, 0.0763, 0.0260, 0.0340, 0.0196,\n",
       "        0.0303, 0.0369, 0.0359, 0.0368, 0.0293, 0.0357, 0.0147, 0.0093, 0.0221,\n",
       "        0.0167, 0.0558, 0.0513, 0.0229, 0.0251, 0.0662, 0.0590, 0.0267, 0.0221],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0687,  0.0913,  0.0201,  0.0489,  0.0185,  0.0763,  0.0260,  0.0340,\n",
       "        -0.9804,  0.0303,  0.0369,  0.0359,  0.0368,  0.0293,  0.0357,  0.0147,\n",
       "         0.0093,  0.0221,  0.0167,  0.0558,  0.0513,  0.0229,  0.0251,  0.0662,\n",
       "         0.0590,  0.0267,  0.0221], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n == F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb25b11e2b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in `dlogits` sums up to $0$ and has a value close to $-1$ at the position of the correct index. The gradients at each cell is like a force in which we're either pulling down on the probability of the incorrect characters or pushing up on the probability of the correct index. This is basically what is happening in each row. The amount of push and pull is exactly equalized because the sum of each row is $0$. Essentially, the repulsion and attraction forces are equal. \n",
    "\n",
    "The amount of force is proportional to the probabilities that came out in the forward pass. A perfectly correct probability prediction $-$ basically zeros everywhere except for 1 at the correct index/position: `[0, 0, 0, 1, 0, ..., 0]` $-$ will yield a `dlogits` row of all $0$ (no push and pull exists). Essentially, the amount to which your prediction is incorrect is exactly the amount by which you're going to get a push or pull in that dimension. <u>The amount to which you mispredict is proportional to the strength of the pull/push.</u> This happens independently in all the dimensions (row) of this tensor. This is the magic of the **cross-entropy loss** and its mechanism dynamically in the backward pass of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"5\"></a>\n",
    "# 5.  Exercise 3: `BatchNorm` Layer Backward Pass\n",
    "------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhpreact.shape, (n*dhpreact).shape, bnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "inside = n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)\n",
    "dhprebn = bngain*bnvar_inv/n * inside\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"6\"></a>\n",
    "# 6.  Exercise 4: Putting it all Together\n",
    "------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7788\n",
      "  10000/ 200000: 2.1985\n",
      "  20000/ 200000: 2.3911\n",
      "  30000/ 200000: 2.4683\n",
      "  40000/ 200000: 1.9580\n",
      "  50000/ 200000: 2.4058\n",
      "  60000/ 200000: 2.3678\n",
      "  70000/ 200000: 2.0933\n",
      "  80000/ 200000: 2.3511\n",
      "  90000/ 200000: 2.1526\n",
      " 100000/ 200000: 1.9435\n",
      " 110000/ 200000: 2.3060\n",
      " 120000/ 200000: 2.0195\n",
      " 130000/ 200000: 2.4472\n",
      " 140000/ 200000: 2.3498\n",
      " 150000/ 200000: 2.2027\n",
      " 160000/ 200000: 1.9351\n",
      " 170000/ 200000: 1.8581\n",
      " 180000/ 200000: 2.0718\n",
      " 190000/ 200000: 1.9001\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the 2-layer MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb] # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        # YOUR CODE HERE :)\n",
    "\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -=1\n",
    "        dlogits /= n\n",
    "\n",
    "        # 2nd linear layer backprop\n",
    "        dh = dlogits @ W2.T # [32 27] * [27 64] -> [32 64]\n",
    "        dW2 = h.T @ dlogits # [64 32] * [32 27] -> [64 27]\n",
    "        db2 = dlogits.sum(0) # [32 27].sum(0) -> [27]\n",
    "\n",
    "        # tanh layer backkprop\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "        # batchnorm layer backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = (1.0 * dhpreact).sum(0, keepdim=True)\n",
    "        _inside = n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)\n",
    "        dhprebn = bngain*bnvar_inv/n * _inside\n",
    "\n",
    "        # 1st linear layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "\n",
    "        # embedding layer backprop\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k,j]\n",
    "                dC[ix] += demb[k,j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # -----------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "    #     if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#     cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0723490715026855\n",
      "val 2.109013319015503\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses achieved:\n",
    "* train: $\\boldsymbol{2.072}$\n",
    "* val: $\\boldsymbol{2.109}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tutorial: loss achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "ambrilli.\n",
      "kimrix.\n",
      "taty.\n",
      "sacassie.\n",
      "mahnen.\n",
      "delynn.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiivon.\n",
      "leigh.\n",
      "ham.\n",
      "join.\n",
      "quinthorline.\n",
      "liveni.\n",
      "waythoniearisi.\n",
      "jace.\n",
      "pirsan.\n",
      "edde.\n",
      "oia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "        logits = h @ W2 + b2 # (N, vocab_size)\n",
    "        # sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><a id=\"7\"></a>\n",
    "# 7. Conclusion\n",
    "---\n",
    "\n",
    "The summary of this notebook was to implement backpropagation manually to calculate the gradients for all the parameters of the neural network. Essentially, we build the full backpropagation through the neural network by hand step-by-step $-$ from the cross entropy loss, 2nd Linear layer, tanh layer, batchnorm layer, 1st Linear layer, to the embedding table $-$ instead of calling Pytorch autograd's `loss.backward()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<br><br>\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## A1. Linear Layer Manual Backpropagation: Simpler Case<a id=\"a1\"></a>\n",
    "-----\n",
    "\n",
    "Let's go through a simple case of `logits = h @ W2 + b2` which we can use to implement the 2nd linear layer gradients: `dh`, `dW2`, `db2`. This is also applicable to the 1st linear layer `hprebn = embcat @ W1 + b1` and its gradients: `dembcat`, `dW1`, `db1`. ![](_imgs/dh_dW2_db2_broadcasting3.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## A2. Broadcasting Examples: Simpler Cases<a id=\"a2\"></a>\n",
    "-----\n",
    "\n",
    "Apply for `dcounts_sum_inv`\n",
    "```\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "c = a * b, but with tensors\n",
    "a[3 x 3] * b[3 x 1] ---> c[3 X 3]\n",
    "|a11*b1 a12*b1 a13*b1|\n",
    "|a21*b2 a22*b2 a23*b2|\n",
    "|a31*b3 a32*b3 a33*b3|\n",
    "```\n",
    "\n",
    "Apply for `dcounts`\n",
    "```\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "|a11 a12 a13|   ---> b1 (= a11 + a12 + a13)\n",
    "|a21 a22 a23|   ---> b2 (= a21 + a22 + a23)\n",
    "|a31 a32 a33|   ---> b3 (= a31 + a32 + a33)\n",
    "```\n",
    "\n",
    "Apply for `dlogits`\n",
    "```\n",
    "norm_logits = logits - logit_maxes\n",
    "|c11 c12 c13|   = a11 a12 a13   b1\n",
    "|c21 c22 c23|   = a21 a22 a23 - b2\n",
    "|c31 c32 c33|   = a31 a32 a33   b3\n",
    "\n",
    "so eg. c32 = a32 - b3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<br><br><a id=\"r1\"></a>\n",
    "# References\n",
    "----\n",
    "1. \"<u>Building makemore Part 4: Becoming a Backprop Ninja</u>\" [youtube video](https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6s), Oct 2022.\n",
    "2. Andrej Karpathy **Makemore** [github repo](https://github.com/karpathy/makemore).\n",
    "3. Andrej Karpathy  **Neural Networks: Zero to Hero** [github repo](https://github.com/karpathy/nn-zero-to-hero/tree/master) ([notebook](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb) to follow video tutorial with).\n",
    "4. Article: \"<u>Become a Backprop Ninja with Andrej Karpathy</u>\" - Kavishka Abeywardana, Pt [1](https://medium.com/@kdwa2404/become-a-backprop-ninja-with-andrej-karpathy-part-1-c0dab9867654), [2](https://medium.com/@kdwa2404/become-a-backprop-ninja-with-andrej-karpathy-part-2-e920cf3ea14c), March 2024.\n",
    "5. \"<u>Yes you should understand backprop</u>\" - Andrej Karpathy, [article](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b), Dec 2016.\n",
    "6. \"<u>Bessel's Correction</u>\" - Emory University Dept. of Mathematics & Computer Science, [academic blog](https://math.oxford.emory.edu/site/math117/besselCorrection/), Dec 2016.\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "45e84815111447888c3e66f0f74ec47f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "8149c6e052904e548f8ccaa70de7d1e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "x0",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_c4a1ac6edc18412c9e497c0a28d14451",
      "max": 30,
      "min": -30,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.5,
      "style": "IPY_MODEL_45e84815111447888c3e66f0f74ec47f",
      "value": 0
     }
    },
    "96e3912e10da470fbcc666223aaed74e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8149c6e052904e548f8ccaa70de7d1e3",
       "IPY_MODEL_c4719115fe714a4dbe0f1bfc8ec439aa"
      ],
      "layout": "IPY_MODEL_a2bbdfe862124bedb33e542da4c21b48"
     }
    },
    "a2bbdfe862124bedb33e542da4c21b48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b23439d8a77a4822ab3304422689123b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4719115fe714a4dbe0f1bfc8ec439aa": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_b23439d8a77a4822ab3304422689123b",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAE/CAYAAACEto0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3ykV33v8c8ZjXqXtmvVbK8b4AJrMDbNobgAJgQu4NDMBRwICVwC5AYIhJIC5iaEECDApSSBUK5pNtgBTC+2sU0x2NjGtqSVdrXlGXVp6vOc+8eRkHZXZUbzTNN836/XvCTNPHrmaDSa+el3fud3jLUWEREREclfpNQDEBEREdkqFFiJiIiIhESBlYiIiEhIFFiJiIiIhESBlYiIiEhIFFiJiIiIhESBlUiFM8bcbYx5UqnHIaszxlhjzGmlHoeIFIcCK5EKZ619mLX2+4W+H2PMp40xf1vo+1nn/p9njPmpMWbBGPP9DY59kjEmMMbMrbi89IRjXmCM+a0xZt4Y86Ax5vGbGFNeQZMx5v8YY35njJk1xtxrjHnJOsdeYoz5tTFmyhgTM8Z8xRjTs+L2HmPM14wxE8aYMWPMqzY7LhHZPAVWIlIpJoB/Bt6T5fGHrLUtKy7/vnSDMeapwHuBlwGtwBOAh8IecBbmgWcC7cBLgQ8YYy5a49h7gEuttR3AHuB3wEdW3P4ZYAjYCTwd+HtjzCWFGriIrE6BlUiFM8YMG2Oesvj5O4wxXzTG/MdiFuRuY8z+E459szHmHmPMpDHmU8aYhsXbrjbG/PiEc1tjzGnGmGuAFwJ/uZj9uWGNsVhjzJ+uyMK82xhz6mKmaWZxbHUb3d9q57bW3myt/SJwKI+Ha8k7gXdZa2+11gbW2oPW2oNr/EynGWN+YIyZNsZ4xpgvLF7/w8VDfrX4mDx/8fo3GWPGjTGHjDH/c71BWGv/xlp77+IYbgN+BDx2jWOPWGtX/uw+cNrifbYATwL+zlqbttb+CrgOWPf+RSR8CqxEtp4rgc8DHcD1wL+ecPsLgUuBU4HTgb/e6ITW2o8BnwWuXcz+PHOdwy8FHgVcCPwl8DHgRUAv8HDgqlx+mDzsMMYcMcYMGWPeb4xpBjDG1AD7ge3GmAcWp83+1RjTuMZ53g18C+gE9gIfBLDWPmHx9nMXH5MvGGMuA94IPBXYBzwl28Eu3v8FwN3rHNNnjJkC4ov3c+3STSd8XPr84dnev4iEQ4GVyNbzY2vtjdZaH/hP4NwTbv9Xa+2otXYC+DvCD3SutdbOWGvvBn4DfMta+5C1dhq4CTg/5Ptbzb3AecBu4A9wgd4/Ld62E6gFngs8fvG481k7wEwD/cAea23CWvvjNY4DeB7wKWvtb6y188A7chjzvwG/Ar651gHW2gOLU4HbFsd77+L1s8BPgLcZYxqMMY8EngM05XD/IhICBVYiW8/hFZ8vAA3GmOiK60ZXfD6Cq9cJ05EVn8dX+bol5Ps7ibX2sLX2nsUptiFc5uw5K8YA8EFr7bi11sMFXVescbq/xGV/frY4tbre9NoeTn58N2SMeR8uu/Q8a63d6PjFoPjfga+t+N2+EBhcvP+P4GquxrK5fxEJT3TjQ0Rki+ld8XkfyzVL86zIcBhjdp3wfRu+4edoo/sLk2XxH0lr7aQxZozjf541fzZr7WHglYtjfBxwszHmh9baB1Y5fJyTH991GWPeCVwOPNFaO7PR8StEgR1AGzBhrR0BnrHivP8F/CyH84lICJSxEqk+rzHG7DXGdAFvBb6weP2vgIcZY85bLGh/xwnfdwQ4JcRxbHR/xzHG1CweFwUii1NetWsce4kxpt84vbiVhF9bccingD83xuwwxnQCrwe+vsa5/ocxZu/il5O4ICxY/PrEx+SLwNXGmLONMU3A32zwM70Z+GPgKdba2AbH/pEx5gxjTMQYsx2XZfvFYvYKY8xZxphWY0ydMeZFwNNYnv4UkSJRYCVSff4LV4z9EPAg8LcA1tr7gXcBN+OW8p9YS/QJ4OzFPkpfzXcQWdzfiV6Mm8b7CK42Kg58fOnGxZV5S72ozgd+isuK/RT4NfDaFed6N3A7cD/wW+AXuHqz1VwA3GaMmcMtBnidtXapNcM7gH9ffEyeZ629CdcS4rvAA4sf1/P3uKzWA2a539Zb1viZeoD/BmYXf54AePaKc12K+51OAq8CLrPWHtvg/kUkZCaL6XwR2SKMMcPAK6y1N5d6LCIiW5EyViIiIiIhUWAlIiIiEhJNBYqIiIiERBkrERERkZAosBIREREJSckahG7bts0ODAyU6u5FREREsnbnnXd61trtGx1XssBqYGCAO+64o1R3LyIiIpI1Y0xWW1RpKlBEREQkJAqsREREREKiwEpEREQkJAqsREREREKiwEpEREQkJAqsRKSoHnwQ/vRPoa0NIhH38U//1F2/1c8pa9PjLeuppOdHyba02b9/v1W7BZHqctNN8NznQjrtLktqa93luuvg8su35jllbXq8ZT3l8vwwxtxprd2/4XEbBVbGmE8CzwCOWmsfvsrtBvgAcAWwAFxtrf35RneswEqkujz4IJxzDiwsrH1MUxPcdReceurWOqesTY+3rKecnh/ZBlbZTAV+GrhsndsvB/YtXq4BPpLNAEWkuvzjPx7/3+Zq0ml4//u33jllbXq8ZT2V+PzIairQGDMAfH2NjNVHge9baz+3+PV9wJOstePrnVMZK5Hq0tYGs7Mrr5kHEkD3ScdNT2/2nDNAAHSEeM4EMAd0AjWbOqes7eTH2wdiuOeFHu9qd/LzIwa0APUnHVfo50eYGauN9ACjK74eW7xutUFdY4y5wxhzx7Fjx0K4axGpFHNzJ14zwfEvHWsdl8s5jwKHQj7nNDAC2A2Ok81Y+3mR3OA4qQbH/97ncX+LJ+dtyun5UdRVgdbaj1lr91tr92/fvuE+hiKyhbS0nHhNhtW2Kz35uFzOmeLE/2TzP2ccqOXEseZyTlnbyY+jBzQtXtY7TqrB8u89AIZwf4u96xxXemEEVgc5/qfcu3idiMjvvehFbgXPspMDq9paePGL8zlnEqgL+ZxxoDGvc8rajn+8FxYv2447Ro939Vp+fixlMQdYOUUM5ff8CCOwuh54iXEuBKY3qq8SkerzhjecGLD4rPYC+frXb/acGdx/tScHVps/J7gaq5MDq1zOKWs7/vH2AAN0HXeMHu/q9YY3QDQ6hXtu7AJaTzqm3J4fGwZWxpjPAbcAZxhjxowxLzfGvMoY86rFQ24EHgIeAD4O/GnBRisiFevUU12/maampTfS5YxVba27/rrrclsyvfKc0Whq8dr6EM+ZxAVrDXmdU9a29Hg3NgZEoxOsXCSgx1v6+tK8730jNDQ0EY3uOe62cn1+bBhYWWuvstbuttbWWmv3Wms/Ya39N2vtvy3ebq21r7HWnmqtfYS1Vkv9RGRVl1/u+s1ccw00N2cwJkpbm/v6rrs21+Rv6ZxXX52iuRmMqQvxnPHFczbmfU5Z2+WXw49+NMWzn+3T2rrt95219XjL8PAwF10UcOedg/zJn5jjOq+X6/NDnddFpOistfz85z9nz5497N69O5RzHj16lNHRUc4991yi0ZOL4jdjfHycQ4cOcf755xOJaAewQrr//vtJpVI8/OEndfWRKrX0N93X10c5LHgrZrsFEZGc+L4PQE1NzQZHZi+ZTBKJREILqgDi8Tj19fUKqgosmUwyOzvLtm3bNj5YqkI8HmdsbIyOjo6yCKpyoVcLESm6TCYDEGoQlEqlqK8/udVCPhKJBI2NjRsfKHnxPA9jDN3d3RsfLFteEAQMDQ0RjUbp7+8v9XBypsBKRIquUIFVXV3dxgdmyVpLIpGgoaEhtHPKyay1xGIx2tvbqT1+OaZUqYMHDxKPx+nv7w/1NaJYFFiJSNEVIrBKJpOhBlaJRAJrrTJWBTY9PU06ndY0oAAwMzPD0aNH2bFjB+3t7aUezqYosBKRoluqsQorsPJ9H9/3Qw2s4vE4gAKrAvM8j9raWtra2ko9FCmxTCbD8PAwjY2N9PSsujNeRVBgJSJFt5SxCqt4PZVyPazCrLGKx+MYYzQVWEDpdJrp6Wm6u7sxxpR6OFJiIyMjZDIZBgcHK3rBSOWOXEQqViaTwRgTemAVdsaqoaFBb/gFFIvFADQNKHiex9TUFD09PRWfJVZgJSJFl8lkQq+vgnADK60ILDzP82htbQ19NadUlkQiwejoKK2trezcubPUw8mbAisRKbpMJhNqD6tUKkUkEgltVVkQBCSTSU0DFtDs7CzJZFLZqipnrWV4eJhIJMLg4GCphxMKBVYiUnS+75d1qwUVrhee53nU1NTQ0dFR6qFICY2PjzM/P09fX9+WabehwEpEii7sqUAFVpXF930mJyfp7u6u6CJlyc/c3Bzj4+Ns27aNzs7OUg8nNHpGi0jRFaLGKuzAKhKJqPanQGKxGNZadVqvYr7vMzQ0RH19Pb29vaUeTqgUWIlI0YUZWAVBQCaTCTUIUuF6YXmeR1NTE01NTaUeipTIgQMHSKfTFd9aYTVb66cRkbIXBAHW2opotSDhW1hYIB6Pq2i9ik1MTDAxMcHu3btpbm4u9XBCp8BKRIoq7O1swg6sMpkM6XRaGasC8TyPSCRCV1dXqYciJZBKpThw4AAtLS3s2rWr1MMpCAVWIlJUYQdWYfewUuF64QRBwMTEBJ2dnaG225DKYK1laGgIgIGBgS3bfFeBlYgUVSEyVsaY0JZqK7AqnMnJSXzf1zRglTpy5Ahzc3P09vZu6YUhCqxEpKiWNmAOs8aqrq4utP9+4/E40Wh0y/TUKSee59HQ0EBLS0uphyJFtrCwwKFDh+js7Nzyq0EVWIlIURViKjDsrWxUuB6+RCLB3NycslVVKAgChoaGqK2tpb+/v9TDKTgFViJSVIWYCgx7RaCmAcPneR7GGBWtV6HR0VESiQQDAwNVUVunwEpEimppn8Awpu6staTT6dDqNVKpFL7vK7AKmbWWWCxGe3u7plirzNTUFJ7nsWvXLlpbW0s9nKJQYCUiReX7ftn2sFLhemFMT0+TyWQ0DVhl0uk0IyMjNDU1sWfPnlIPp2gUWIlIUYXZdV2tFiqD53nU1tbS1tZW6qFIEQ0PDxMEAYODg1u2tcJqFFiJSFGFGVgtZazCmgpMJBLU1dVVRR1IsaRSKaanp9m2bVtVvblWu6NHjzIzM8PevXurbjGIAisRKapCBFZh9rCqtjeBQovFYgBbfom9LIvH44yNjdHe3s727dtLPZyiU2AlIkW1VLwehjB7WFlrtSKwADzPo62tbUs3hJRlS60VampqGBgYKPVwSkKBlYgUjbUW3/fLsodVMpnEWqvAKkQzMzOkUikVrVeRQ4cOEY/HGRgYCO3vvNIosBKRolnquh7mVGBYmRAVrofP8zyi0SgdHR2lHooUwczMDEeOHGH79u20t7eXejglo8BKRIomzOagSz2swl4RqBqrcGQyGaampujq6lLRehXIZDIMDw/T0NDA3r17Sz2cklJgJSJFE+Y+gel0GmttaIFVIpGgvr6eSEQvi2GYmJjAWqtpwCoxMjJCJpNhcHCw6v+GqvunF5GiCjNjVYgeVpoGDI/neTQ3N+sxrQKe5zE1NUVPTw9NTU2lHk7JKbASkaIJM7AKs4dVEAQkEgkFASGZn58nHo8rW1UFkskko6OjtLa2snPnzlIPpywosBKRoilEYBVGxiqRSAAqXA+L53lEIhE6OztLPRQpIGstQ0NDGGOqtrXCahRYiUjRZDIZjDGh1Fglk0lqa2tDKYzWisDwBEHAxMQEXV1d6mC/xY2PjzM/P09/f39oU/JbgQIrESmasDdgDrNw3RijJpYhmJiYIAgCdVrf4ubm5hgfH6e7u1uZyRMosBKRogl7O5swe1g1NDSoLUAIPM+joaGBlpaWUg9FCsT3fYaGhqivr6e3t7fUwyk7CqxEpGjCDqy0IrC8xONx5ufnVbS+xR04cIB0Os3g4KCme1ehwEpEiiasfQLD7GHl+z6pVEqBVQhisRjGGE0DbmETExNMTEywe/dumpubSz2csqTASkSKJqx9Apd6WIUxFajC9XBYa4nFYnR0dFTtHnFbXSqV4sCBAzQ3N7Nr165SD6dsZRVYGWMuM8bcZ4x5wBjzV6vc3meM+Z4x5hfGmLuMMVeEP1QRqXRhTQWq1UL5mZqaIpPJaBpwi1pqrWCtZXBwUPWI69gwsDLG1AAfAi4HzgauMsacfcJhfw180Vp7PvAC4MNhD1REKlsQBARBUHaBVTweJxKJaLl4njzPo66ujtbW1lIPRQrgyJEjzM3N0dfXp9WzG8gmY/Vo4AFr7UPW2hTweeBZJxxjgbbFz9uBQ+ENUUS2grC3s4lGo6HsSabC9fylUilmZmbYtm2bMhlb0MLCAocOHaKzs1P1c1nI5lWpBxhd8fXY4nUrvQN4kTFmDLgR+PNQRiciW0aYGzBrRWB58TwPQG+6W1AQBAwNDRGNRunr6yv1cCpCWMXrVwGfttbuBa4A/tMYc9K5jTHXGGPuMMbccezYsZDuWkQqQdjb2YQxHZFOp8lkMgqs8rBUtN7W1qbp1C1obGyMRCLB4OCgFiVkKZvA6iCwsgPY3sXrVno58EUAa+0tQANwUgWjtfZj1tr91tr927dv39yIRaQihR1YhVVfBSpcz8fs7CypVEpF61vQ1NQUx44dY+fOnaqdy0E2gdXtwD5jzKAxpg5XnH79CcccAJ4MYIw5CxdYKSUlIr8XVmCVTqcJgkArAsuE53lEo1E6OjpKPRQJUTqdZmRkhKamJnp6Tqz+kfVsGFhZazPAnwHfBH6LW/13tzHmXcaYKxcPewPwSmPMr4DPAVdba22hBi0ilSesGqulFYFh9bCKRqOa4tikTCbD1NQU3d3dKlrfYoaHhwmCQK0VNiGrVxNr7Y24ovSV1719xef3ABeHOzQR2UoymQyRSCTvlXxht1pQtmrzYrEY1lpNA24xR48eZWZmhr6+PhoaGko9nIqjzusiUhTl2BxUgVV+PM+jpaVFb75bSDweZ2xsjPb2dlQLvTkKrESkKMIKrJLJJDU1NXlPKSaTSYIgUGC1SXNzcyQSCbVY2EKWuqvX1NTQ399f6uFULAVWIlIUYW3AHFarBRWu58fzPCKRCF1dXaUeioTk4MGDxONxBgYGqK2tLfVwKpYCKxEpirA2YA671YKmsXLn+z6Tk5N0dXWF0v1eSm9mZoYjR46wfft22tvbSz2ciqa/CBEpijCnAsMKrOrq6kLJolWbyclJgiBQ0foWkclkGB4epqGhgb1795Z6OBVPgZWIFEUYgVUmkwmth5UK1zfP8zwaGxtpbm4u9VAkBAcOHCCTyTA4OKgMZAj0CIpIwZVbDytrLYlEQoHVJsTjcebn55Wt2iI8z2NycpKenh6amppKPZwtQYGViBRcWF3Xw2q1kEgksNYqsNoEz/MwxqhofQtIJpOMjo7S2trKzp07Sz2cLUOBlYgUXFiBVTKZBMIJrEArAnMVBAGxWIzOzk51q69wS60VjDEMDAyUejhbigIrESm4MDNWkUgk7/PE43GMMVoRmKOpqSl831fvqi1gfHyc+fl5+vv7Q6lZlGUKrESk4MIMrMLaI7C+vl57oOXI8zzq6upoa2sr9VAkD3Nzc4yPj9Pd3U1nZ2eph7PlKLASkYILs3hdKwJLI5lMMjs7q6L1Cuf7PkNDQ9TX19Pb21vq4WxJCqxEpODCrLHKN7AKgoBkMqnAKkexWAxA04AVbnR0lFQqxcDAgHq4FYgCKxEpuDB6WPm+j+/7eU8FqnA9d9ZaPM+jvb1d9TgVbHJyklgsxu7du2lpaSn1cLYsBVYiUnBh7BMYVquFpa1sFFhlb2ZmhnQ6rWnACpZKpRgZGaG5uZndu3eXejhbmgIrESm4MPYJDDOwikQiyrzkwPM8otGo9pCrUNZahoeHsdYyODioRRsFpsBKRAoujKnAsHpYxeNxGhoa9OaSpXQ6zfT0NN3d3XrMKtSRI0eYnZ2lr68vlFW1sj4FViJScGEEVks9rGpra/M6j1YE5iYWi2Gt1TRghVpYWODQoUN0dnZq4UGRKLASkYILK7DKN1uVyWRIp9MKrHLgeR4tLS1qplqBgiBgaGiIaDRKX19fqYdTNRRYiUhBWWsJgiDv4vUwWi1oRWBu5ubmSCaTylZVqLGxMRKJBIODg9qCqIgUWIlIQYXZdT2sFYHKvmTH8zxqamrUnbsCTU9Pc+zYMXbu3Elra2uph1NVFFiJSEGFEVgFQUAmk8m78DYej1NTU6MVgVnwfZ/JyUm6urqIRPRWUUnS6TTDw8M0NjbS09NT6uFUHf21iEhBhRFYhdlqQdOA2ZmYmCAIAk0DVqCRkRGCIOCUU07RSs4SUGAlIgUVxj6BYbZaUGCVHc/zaGpqoqmpqdRDkRwcO3aM6elpenp6NOVdIgqsRKSgwsxY5TMVmE6n8X1fgVUWFhYWWFhY0PL8CpNIJBgbG6O9vZ0dO3aUejhVS4GViBRUWIGVMSavHlYqXM+e53kYYxRYVRBrLQ899BCRSIT+/v5SD6eqKbASkYLKZDJEIpG8CqDDXBGojNX6giBgYmKCzs7OvFtkSPEcPHiQeDzOwMBA3k10JT8KrESkoMLYgDmMHlbxeJza2lr189nA1NQUvu+raL2CzM7OcuTIEbZv3679HMuAAisRKaiwNmAOo9WCslUb8zyP+vp69T6qEJlMhqGhIRoaGti7d2+phyMosBKRAst3OxtrLel0Oq+MlbWWRCKhwGoDyWSS2dlZZasqyIEDB8hkMgwODqrfWJnQb0FECirfwCqMHlapVIogCBRYbUBF65UlFosxOTnJnj171BajjCiwEpGCyrfGKoweVloRuDFrLbFYjLa2NhU/V4BkMsmBAwdobW1l586dpR6OrKDASkQKKt8aqzB6WGlF4Mamp6dJp9OaBqwA1lqGhoYwxjAwMKDu6mVGgZWIFIzv+1hry6KHVX19vWpQ1uF5HrW1tVpVVgHGx8eZn5+nv79f+16WIb3KiEjBhNEcNJlMUltbm9d/5VoRuL50Os309DTd3d3KfpS5ubk5xsfH6e7uprOzs9TDkVUosBKRggmr63q+KwKTyaQCq3XEYjEATQOWOd/3GR4epq6ujt7e3lIPR9agwEpECiaMDZjz7WGVSCSw1qpwfR2e59Ha2pp3rzAprNHRUZLJJIODg+qKX8YUWIlIweSbsbLW5p2xUuH6+mZnZ0kmk8pWlbnJyUlisRi7d++mpaWl1MORdSiwEpGCyTewCqOHVTwexxijjNUaPM+jpqaGjo6OUg9F1pBKpRgZGaG5uZndu3eXejiyAQVWIlIwS4HVZqctwmq10NDQoKLsVfi+z+TkJF1dXVoxWcaGh4ex1jI4OKjncQXI6i/JGHOZMeY+Y8wDxpi/WuOY5xlj7jHG3G2M+a9whykilcj3fWpqajb9ZhBWxkrTgKuLxWJYazUNWMaOHDnC7Owsvb29qoGrEBvm540xNcCHgKcCY8DtxpjrrbX3rDhmH/Bm4GJr7aQxZkehBiwilaPU29n4vk8qlVJgtQbP82hqatJ2KGVqYWGBgwcP0tHRoeC3gmSTsXo08IC19iFrbQr4PPCsE455JfAha+0kgLX2aLjDFJFKlG9glW8Pq0QiAWgrm9UsLCwQj8f1hl2mgiBgaGiIaDRKf39/qYcjOcgmsOoBRld8PbZ43UqnA6cbY35ijLnVGHNZWAMUkcoVRsZKKwILw/M8IpEIXV1dpR6KrGJsbIxEIsHAwEBef0NSfGH9tqLAPuBJwF7gh8aYR1hrp1YeZIy5BrgGoK+vL6S7FpFy5ft+XtmiVCpFc3Pzpr8/Ho8TiURUm3KCIAiYmJigs7NT/ZDK0PT0NMeOHWPnzp20tbWVejiSo2wyVgeBlS1e9y5et9IYcL21Nm2tHQLuxwVax7HWfsxau99au3/79u2bHbOIVIh8MlZh9bBStupkk5OT+L6vacAylE6nGR4eprGxkZ6eEyeHpBJkE1jdDuwzxgwaY+qAFwDXn3DMV3HZKowx23BTgw+FOE4RqTDWWnzf33RglU6nsdYqsCoAz/NoaGhQo8kyNDIygu/7aq1QwTYMrKy1GeDPgG8CvwW+aK292xjzLmPMlYuHfROIGWPuAb4HvMlaGyvUoEWk/IXVHHSz03iZTIZMJqPC9RMkEgnm5ubo7u4u9VDkBMeOHWN6epq9e/fqH4IKltUrnrX2RuDGE657+4rPLfAXixcRkbz3Ccy31YIK11fneR7GGAVWZSaRSDA2NkZbWxs7dqhjUSVTq10RKYh8M1bJZBJQYBUmay2xWIz29nZqa2tLPRxZZK1laGiISCTCwMBAqYcjeVJgJSIFEcZUYDQa3fRWK/F4nGg0qgBihenpaTKZjIrWy8zBgwdZWFigv79fz9ctQIGViBREGIFVvnsEKlt1PM/zqK2t1RL+MjI7O8uRI0fYvn27NsLeIhRYiUhBhFFjlc+KwEQiocL1FVKpFNPT02zbtk2rzcpEJpNhaGiIhoYG9u7dW+rhSEgUWIlIQWQyGYwxJQmsUqkUvu8rY7VCLOYWaqtovXwcOHCATCbD4ODgpqe8pfzoNykiBZFPc9B0Ok0QBCpcD5HnebS2tqoLfZmIxWJMTk6yZ88ebYK9xSiwEpGCyCewyreHlQKr483MzJBKpVS0XiaSySQHDhygpaWFnTt3lno4EjIFViJSEJlMpqQ9rOrq6rQP3iLP84hGoyqOLgNLrRWMMequvkUpsBKRgshnO5swelgpW+VkMhmmpqbo6upSHU8ZOHz4MPPz8/T19eW1OEPKl/7KRKQg8p0KrKmp2VTGyVqrFYErTExMYK3VNGAZmJ+fZ3x8nK6uLrq6uko9HCkQBVYiUhD5Blabra9KJpNYa5WxWuR5Hs3NzXo8Ssz3fYaGhqitraWvr6/Uw5ECUmAlIqELggBrbUlaLahwfdn8/DzxeFzZqjIwOjpKMplkcHBQtX9bnAIrEQldGPsE5htYaSrQZasikQidnZ2lHkpVm5ycJBaLsXv3blpaWko9HCkwBVYiErp8AqtMJkMQBBgCBmcAACAASURBVHm1WmhoaKj6Qu0gCJiYmKCzs1MZkhJKpVKMjIzQ3NzM7t27Sz0cKYLqfuURkYLIJ7DKt9WCCtediYkJgiDQNGCJDQ8PY61Va4UqosBKREJXqsAqCAISiYTqq3DTgA0NDZp6KqEjR44wOztLb2+vOt5XEQVWIhK6fDZgzqeHVSKRAFS4nkgkmJ+fV7aqhBYWFjh48CAdHR36PVQZBVYiErp8M1Y1NTWb+l6tCHQ8z8MYow2XSyQIAoaGhohGo/T395d6OFJkCqxEJHRL29lspqYk31YLxpiqnnax1hKLxejo6Nj0qkzJz9jYGIlEgoGBAf0OqpACKxEJXT77BObbaqGxsbGqi4SnpqbIZDKafiqR6elpjh07xo4dO2hrayv1cKQEFFiJSOjy2Scwn4yVVgS6acC6ujpaW1tLPZSqk8lkGB4eprGxkZ6enlIPR0pEgZWIhG6z29n4vo/v+5uayvN9n1QqVdX1ValUipmZGbq7u6s6a1cqw8PD+L7P4OBg1fdRq2b6zYtI6DYbWOXTakGF6y5bBWgasASOHTvG9PQ0e/furernoCiwEpEC2GxglU+rhWoPrJaK1tva2jY9lSqbk0gkGBsbo62tjR07dpR6OFJiCqxEJFTWWnzf31Tx+lLGajNTgfF4nJqamqoNKmZnZ0mlUspWFZm1lqGhISKRCAMDA6UejpQBBVYiEqql5qCbnQqMRCKb+t5qL1z3PI9oNEpHR0eph1JVDh06xMLCAv39/dTW1pZ6OFIGFFiJSKjybQ6ab6uFapTJZJiamlLRepHNzs5y+PBhtm3bpoBWfk+BlYiEKp/AarM9rNLpNJlMpmoDq1gshrVW04BF5Ps+w8PDNDQ00NvbW+rhSBlRYCUiocpnn8BUKrXp+iqo3sJ1z/Nobm6u6qnQYhsZGSGdTqu1gpxEzwYRCdVmM1ZBEJDJZLQiMEdzc3MkEgllq4ooFosxOTnJnj17aGpqKvVwpMwosBKRUG02sMqnh1UikSAajVblvmye5xGJROjq6ir1UKpCMplkdHSUlpYWdu7cWerhSBlSYCUiocpkMhhjcp4KzLeHVTVmq3zfZ3Jykq6uLk1HFYG1luHhYQAGBwe1UEBWpb9EEQnVZjdgzreHVTUGVpOTkwRBoGnAIjl8+DBzc3P09fVVbb802ZgCKxEJ1WY3YE6lUhhjcu4FlEwmCYKgKgMrz/NobGykubm51EPZ8ubn5xkfH6erq0vTrrIuBVYiEqp8trNR4Xr24vE48/PzylYVge/7DA0NUVtbS19fX6mHI2VOgZWIhCqfDZjzCayqrdWA53kYY5Q9KYKxsTGSySSDg4ObmuaW6qLASkRClU+N1WbqqxKJBHV1dVX1hhcEAbFYjI6OjqpcCVlMk5OTeJ7Hrl27aGlpKfVwpAIosBKRUG2mxioIAtLptFYEZmlqagrf9zUNWGDpdJqRkRGamprYs2dPqYcjFUKBlYiEJggCgiAoWg8ray2JRKLqAivP86irq6Otra3UQ9nShoaGsNaqtYLkRIGViIQm3+aguU4FJhIJrLVVFVglk0lmZ2eVrSqwI0eOMDs7S29vb9XV70l+FFiJSGiK3XW9GlcExmIxALq7u0s8kq0rHo9z8OBBOjo6FMBKzrIKrIwxlxlj7jPGPGCM+at1jnuOMcYaY/aHN0QRqRSb3YB5sz2sEokExpiqyShYa/E8j/b2djWoLJAgCHjooYeIRqP09/eXejhSgTYMrIwxNcCHgMuBs4GrjDFnr3JcK/A64LawBykilWGzGatkMkltbW3OdSzxeJz6+vqqqX+ZmZkhnU4ri1JABw8eJJFIMDAwoBWXsinZZKweDTxgrX3IWpsCPg88a5Xj3g28F0iEOD4RqSD5TAVqK5uNeZ5HNBqlvb291EPZkqanpzl69Cg7duzQwgDZtGxe/XqA0RVfjwGPWXmAMeaRQK+19hvGmDeFOD4RqSD5BFatra05fU8wM0Pye9+j+957YXQUrD35oMFBuOQSeOITocIbaabTaaanp9mxY0fVZOiKKZPJMDIyQmNjIz09PaUejlSwvPOcxpgI8E/A1Vkcew1wDaBtAUS2IN/3iUQiOb3xW2uz67puLfzoR/Ctb8F3v0v8Zz8D36cxGoWzz4YTg7kggO98Bz74QTAGzj3XBVlPfjJceunJx5e5WCyGtVbTgAUyPDxMJpNh3759RCJa1yWbl80ry0Ggd8XXexevW9IKPBz4/uKL6S7gemPMldbaO1aeyFr7MeBjAPv371/l30sRqWSb2c5mwxWB1sI3vgHvfCfccQfU1MD+/cRf8xo4/XQar7pq7WxUKgU/+xl873vw3e/Chz8M738/nHYa/PVfwwtfWDEBlud5tLS0VE2hfjEdO3aM6elpent7q2pqWQojm7D8dmCfMWbQGFMHvAC4fulGa+20tXabtXbAWjsA3AqcFFSJyNaXT2B1Uo2VtXD99XDBBfDMZ0IsBv/3/8LEBNx6K4k3vYnIRRdR19m59snr6uBxj4O3vc0FV1NT8KUvQUsLXH01nHkmfPrTsDiFWa7m5uZIJpPKVhVAIpFgbGyMtrY2duzYUerhyBawYWBlrc0AfwZ8E/gt8EVr7d3GmHcZY64s9ABFpHKElrH69rfhUY+CZz0LJifhk5+E++6Dl78cFouK4/E4DQ0NudUbNTTAH/0R/Pzn8LWvuXO97GVwxhnwmc+sXqdVBjzPo6amhs71gkjJmbWWoaEhIpEIAwMDpR6ObBFZTSRba2+01p5urT3VWvt3i9e93Vp7/SrHPknZKpHqtJkNmJPJJLAYWCUS8LrXwdOeBjMz8KlPwb33uuDnhB5Xea0INAauvBLuvNNlxTo64MUvhv/xP1xGrIz4vs/k5CRdXV2q/QnZoUOHWFhYoL+/P+ceaiJr0V+piIRmMxswp1Ip18Pqt7+FxzwG/uVfXHD1m9+46bpV3vAymQzpdDr/ehhj3DTj7bfDtde6LNa558IPf5jfeUM0MTFBEASaBgzZ7Owshw8fZtu2bXR0dJR6OLKFKLASkdBsaiowmaT+K1+B/fthfNwVqv/zP7tpuzWEvpVNJAJvehPccou730sucXVZZVB75XkejY2NNDU1lXooW4bv+wwPD1NfX09vb+/G3yCSAwVWIhKKTfWwmpwk9ZrXUPeOd8DjHw933QVXXLHhtyUSrg9x6Cu49u939VcveQn87d/CE54AIyPh3kcOFhYWWFhYULYqZAcOHCCdTjM4OKjpVQmdnlEiEoqc9wk8eBD7+MeT+sEPqHv72+Gmm2DXrqy+NR6PU1NTU5i6mNZWV9v1uc/B3XfDYx/rAr4S8DwPY4w2XA7RxMQEExMT7N69m+bm5lIPR7YgBVYiEoqcMlb33QcXX0x6ZAT7wQ9S/+d/7qbjslSUrWxe8AL46U/duJ7wBNectIiCIGBiYoLOzs6cFwTI6pLJJAcOHKClpYVdWQbxIrlSYCUiocg6sLr9dtdbamGB1E03wQUXbNx1/QRF2yPwYQ9zwdWuXW6l4vUnLYQumKmpKXzf1zRgSKy1DA8PAzA4OKhtgaRgFFiJSCiyCqy+/W1XGN7SAj/5CamzzwbW6bq+ilQqhe/7xeuQ3dcHP/4xPOIRrgfWpz5VlLv1PI/6+vqc91CU1R0+fJi5uTn6+vpyDuRFcqHASkRCsWFg9YUvwNOfDqee6rJA+/Yd38MqSwUrXF/Ptm1uS5w/+AP4n/8T3vvegt5dMplkdnZW2aqQzM/PMz4+TldXF10Vvhm3lD8FViISinWL17/wBbjqKrjwQvjBD2D3bsBln6LRaE4rs5ZaLRR9z7yWFvj6113t1V/9FbznPQW7K8/zAFS0HoIgCBgaGqK2tpa+vr5SD0eqQGXsPioiZW/NHlY33QQvehFcfDH893/Din5MqVTq5D0CNxCPx6mtrc25X1Yo6urgs591Be1vfjO0t8OrXx3qXVhricVitLe3qxt4CEZHR0kmk5x++ulaBCBFocBKREKxamD1ox/Bc57j6pO+/vXjgipwU165Nr4sWuH6WiIRt3HzzAy85jUuuPrjPw7t9NPT06TTaU0DhmBqagrP89i1a5dq1aRoNBUoIqE4aZ/An/8cnvEMV/z9zW+6AOQEqVQqp/oqay2JRKK0gRW4bXa++EV44hNdM9Ebbgjt1J7nUVtbS/sqj5dkL51OMzIyQlNTE3v27Cn1cKSKKLASkVAct0/gvffCpZe6zY2//W3Yvv2k49PpNNbanKYCk8kkQRCUPrACaGx07Rce+Ui3efP3v5/3KdPpNNPT03R3d6sdQJ6Gh4cJgkCtFaToFFiJSCh+PxU4MgJPfSrU1MDNN8Mae7GlUimgAlYErqe11dWQnXrq8mbOeYjFYgCaBszTkSNHmJmZYe/evcVf5CBVT4GViIQik8kQnZ+Hyy+HuTn41rdg3741j99MYFWyFYHr6e5ezspdcQU89NCmT+V5Hq2trTkX9MuyeDzOwYMH6ejoYPsqmVKRQlNgJSJ5C4KAIJUi+upXwwMPwFe/Cuecs+73bKaHVTwep76+vvw2zt2zx9WR+b6rK5uezvkUs7OzJJNJZavysNRaIRqN0t/fX+rhSJUqs1cnEalEfiYD115LzY9+BB/9qCvq3sBSD6tclsCXfEXgevbtgy9/GX73O3je82CxYWq2PM+jpqaGjo6OAg1w6zt48CDxeJyBgYHStOMQQYGViIQg88//DF/+MtHXvQ5e9rKsvifXFYFBEJBMJss3sAJ40pNcYPmtb8FrXwvWZvVtvu8zOTlJV1dX+WXjKsTMzAxHjx5lx44dtLW1lXo4UsUU0otIfr7+dTL/+3/DJZcQfec7s/62ZDKZU61UMpnEWlvegRW4LW/uuw+uvRbOPNMFWBuIxWJYazUNuEmZTIbh4WEaGxvp6ekp9XCkyulfIxHZvF/9Cl7wAjLnngvvehfRHDdTrvjC9bX8wz/AH/4hvP718I1vbHi453k0NTXl3CxVnJGRETKZDIODg8r4ScnpGSgim3P4sGsx0NGB/5nPQGNj1vVSmUyGIAhyWv0Wj8cxxlRGYBWJwGc+A+ed5/YW/PWv1zx0YWGBeDyubNUmeZ7H1NQUPT095Z/NlKqgwEpEcpdKwXOfC54HN9xAZjEoyLZgeLOtFhoaGiqn2WNzs2sg2tbmslcTE6se5nkekUiErq6uIg+w8iUSCUZHR2lra2Pnzp2lHo4IoMBKRDbj9a+Hn/wEPvlJOP98MpkMkUgk62mYzbZaqLiMRE8PfOlLMDbm9hP0/eNuDoKAiYkJOjs7tUFwjqy1DA0NEYlEGBgYKPVwRH5PgZWI5OaTn4QPfxje9CY3zcUaGzCvYyljle1UoO/7pFKpygusAC68ED70Idfn6q//+ribJicn8X1f04CbcOjQIRYWFujv76e2trbUwxH5Pa0KFJHs/exn8OpXw1OeAn//97+/+qQNmDeQSqWoqanJ+nvKbiubXL3iFXDHHfCe9yzvLYibBqyvr6elpaXEA6wsc3NzHD58mG3btqnvl5QdZaxEJDtHjsAf/ZHrMv75z8OKDNVxGzBnYUuvCFzLBz4Aj32s6/P1m9+QSCSYm5tTtipHvu8zNDREfX09vWvsQylSSgqsRGRj6bTrJj4xAV/5itsfb4VcpwKTyWTOgVUkEqnsPfTq6+G669zGzX/4h3gPPIAxhu4THktZ34EDB0in02qtIGVLz0oR2dgb3gA//CF84hOuhcAJNlNjlWurhYqdBlxpzx740pewIyPEXvUq2ltaVB+Ug4mJCSYmJti9ezfNzc2lHo7IqhRYicj6PvtZ+OAH4S/+Aq66atVDfN/Pul7K931839/6KwLXctFFTF97LZmf/IRtn/hEqUdTMVKpFAcOHKClpYVdu3aVejgia1JgJSJru+sueOUr3abK733vqof4vo+1tmA9rDKZDJlMZusEVoD3jGdQe+WVtF17bVad2avdUmsFgIGBgcrpZSZVSYGViKxuagqe8xzo6DipWH2lTCYDZN8cNNceVluicH2FVCrF9MwM2/7pnzDnnQcvehE89FCph1XWDh8+zNzcHL29vZVdZydVQYGViJwsCOAlL4HhYfh//w/WmXrJNbDKtYfVUmC1VTJWsVgMgO69e13zUHAB7OLPKcebn59nfHyczs5OFfpLRVBgJSIne8974IYb4B//ES6+eN1DNxNYRSKRrI+Px+NEo9EtU+TteR6tra0usDzlFFfD9stfuv5g1pZ6eGUlCAKGhoaora2lv7+/1MMRyYoCKxE53re/DW97mytU//M/3/Bwf3GblmyL1zfTamGrZKtmZmZIpVLH96664gp4+9vh3/8dPvax0g2uDI2OjpJMJhkYGNCWP1IxFFiJyLIDB1xAddZZ8PGPQxZFwpvJWFVrYOV5HjU1NSd3C3/72+Gyy+C1r3Xd7YWpqSk8z2PXrl20traWejgiWVNgJSJOIuFqfVIp+PKXIcs+QUuBVbYZhVx6WKVSKYIg2BKBVSaTYWpqiu7u7pMbW9bUuCnBPXvc7+DYsdIMskyk02lGRkZoampiz549pR6OSE4UWImI89rXuv3s/uM/4PTTs/62pX0Cs1kCHwQBmUymKlcETkxMYK1dewubri5XzO55bnPrxYC1Gg0PDxMEAYODg2qtIBVHgZWIuI7qH/84vPnN8Id/mNO35rJP4GZbLWyFjJXneTQ3N6//szzykfCRj8B3vwtvfWvxBldGjh49yszMDHv37t0SAbVUHwVWItXujjvgNa+BpzwF3v3unL89l+1sNtNqoa6uruILl+fn54nH49ltuHz11fAnfwLXXrvcjqFKxONxxsbGaG9vZ/v27aUejsimKLASqWae52p6du6Ez33O1frkaDOBVS4Zq62SrYpEInR2dmb3DR/4ADzmMS7Iuvfego6tXCy1VohGowwMDJR6OCKbllVgZYy5zBhznzHmAWPMX61y+18YY+4xxtxljPmOMUYNR0TKne+7FYBHjrjMSDbZlFXkGlgZY7LqSWWtJZFIVHxgFQQBExMTdHZ2Zp95q6+H666DxkZ49rNhdrawgywDBw8eJB6P09/fn9OG3iLlZsPAyhhTA3wIuBw4G7jKGHP2CYf9AthvrT0HuA64NuyBikjI3vY2uPlm+NCHYP/+TZ8mlw2Yc+lhlUwmsdZWfJ3NxMQEQRBkNw240t698IUvwP33w8tetqWbh87MzHD06FF27NhBe3t7qYcjkpdsMlaPBh6w1j5krU0BnweetfIAa+33rLULi1/eCuwNd5giEqovfQn+4R/cBssvf/mmT2Otzal4PZdWC1ulcN3zPBoaGmhpacn9my+5xG1+/aUvrbkJdqXLZDIMDw/T0NBAT09PqYcjkrdsAqseYHTF12OL163l5cBN+QxKRArorrvgpS+FCy+ED34wr1MVsjnoVmi1kEgkmJ+fzz1btdIb3gDPfz685S1w443hDa5MjIyMkMlkGBwcPLm/l0gFCvVZbIx5EbAfeN8at19jjLnDGHPHsSpvgCdSEp4Hz3oWtLe7JqBZZo/WkktgFQQB6XQ6p8CqoaGhot9sPc/DGJPf5sHGwCc/Ceed52ri7rsvvAGWmOd5TE1N0dPTQ1NTU6mHIxKKbF6xDgK9K77eu3jdcYwxTwHeClxprU2udiJr7cestfuttfu1lFakyNJpeN7zYHwcvvIV2L0771Pmsk9gta0ItNYSi8Xo6OjIvxi7qQm++lUXCD/rWTA9Hc4gSyiRSDA6Okprays7d+4s9XBEQpNNYHU7sM8YM2iMqQNeAFy/8gBjzPnAR3FB1dHwhykieXvjG+F733Mb/T760aGcMpeMVS49rIIgIJlMVvQ04NTUFJlMJr9pwJX6+txKwQcfhBe+0K3qrFDWWoaHhzHGqLWCbDkbBlbW2gzwZ8A3gd8CX7TW3m2MeZcx5srFw94HtAD/zxjzS2PM9WucTkRK4ZOfhH/5F3j96+ElLwnttJsJrLLJWCUSCaCyC9c9z6Ouri7cDYSf8AT3e/zGN9yqzgo1Pj7O/Pw8/f39OW3ILVIJsspPW2tvBG484bq3r/j8KSGPS0TCcsst8OpXu87q14bbCSWXwCqZTGbdw6rSVwSmUilmZmbYvXt3+HvdvepV8MtfulWd557rCtsryNzcHOPj43R3d2ffMFWkglRuVaiIbGxkxDWYXOqJFHLjRd/3McZkVWCeSqWora3NKtCIx+MYY7JuzVBuPM8DCG8acCVj3GrOiy92/a1+9rPw76NAfN9naGiI+vp6ent7N/4GkQqkwEpkq5qehqc/HRIJuOEG6OoK/S5y7bqeSw+rxsbG8LM9RbBUtN7W1la4aa66Oreqc9cueOYzYXi4MPcTsgMHDpBOpxkcHKz4/R9F1qJ9A0S2onQanvtctzT/v/8bzj5xs4Rw5BpYZVtvFI/HaWtry2doJTM7O0sqlWLv3nD7JCeTMDPjLrOzMDOzA/uX3+Cxb7yIhcc9na++8SdMmw4SCUil3FMgkzn+Ai7htfTRGIhEXJxWW+sudXXu0tAAzc1uQeLSpbkZ2tpct472dndMtrHvxMQEExMT7Nmzh+bm5lAfG5FyosBKZKux1tVU3XyzK1p/8pMLdlfZBlbW2qybg/q+TzqdrtgVgZ7nEY1G6ejoWPMY34djx9w2jUuXo0fdR8+DiQmIxdzHpcti7f8JzuJJfJlvzl/K3tc/l2u4kTTLj3E06oKlaPT4/bWtXd4hx/ddELb6+ddXW+sCrM5O6O52201u27b8+c6dLqnW3Z1ievoAe/a0sGvXrtzvSKSCKLAS2Wre+174xCfgrW91NTgFlMlksgqAcmm1UMmF65lMhomJKYzZwW23GUZGXJnb2BgcPLh8GR9fvVtCff1yYNLVBWed5T52dkJHB7S2uozR0qWlBRobL2Hypo/zlP99NTMvfDXBR/8v9Q2GSCT7bBK4QMv3XYCVSrkZ5IUFmJ93HxcWYG7OZcymp4+/TE66QHBsDH71KxccLv4aAQsMLX4+wI4dhr17OenS1weDg9DTc3wQKFJpFFiJbCVf/CK8+c3wghfAu95V8LvLdp/AXFotVEJgNT8PDz3kWkqtvPzudzHGxizp9PGd1tvaXMDQ0+MSiD09sGePy+bs2OEyOzt3uuM2VVb2iJfC3IM0vPvdcPapbvubHBnjMlvRqJv2y9fCgsvC3X33Ee69dw5rB5ibq+fQIRdcDg3Bj37kgrKVolHo7XVB1sAAnHaau5x6qvtYoTPEUkUUWIlsFT/9qetRdfHF8KlPueKZAst2KjDXwKqmpqbk/Y2CwNWE33efu9x///LnB0/Ye6Kjw73x79vnccklzTzykY3097vAoK+vSMHAO9/poru3vtXd8R//cRHudG1NTbBjxwITE4c466xOTjll9W195uddpuvAAfd4Dw+7oGt42G2NePjw8cdv3w779sEZZ8CZZ7rLGWfAKae4qUmRUlNgJbIV/OpXbgVgb6/b+qQI9Um+72OtzbqHFWQfWBUzW+X7Lvt0991wzz3Ll3vvXTmd5YKnM85wGad9+5azKKee6qbr5ubmuO++BP39/RSiy8KGlvYUHBtzm2x3dsLll5dgIE4QBAwNDVFbW0t/f/+axzU3u8f1jDNWv31uzv1+Hnhg+XL//XDTTe7/hyXRqPu9POxhx1/27VPAJcWlwEqk0v3ud/C0p7mCm5tvpljv6rnuE5hLD6tCNY70PBeD3nUX/PrX7nL33ccHUH19bhHlJZe4GqezzoLTT3cP63rD9zyPSCRCVwHaWmStvh6uv94N/jnPgW9+Ex7/+JIMZXR0lEQiwemnn55Xa4WWFjjnHHc50dTUchbxt791l1/+Er70peXi/Npal9U65xx4xCOWz7VnzyanXUU2oMBKpJKNjbmO6kEA3/42rJMZCFuu29lkU7ieTqfxfT/vjFUQuCzHL395/GXlFN6OHe6N9k/+BB7+cHc5+2xXIJ4r3/eZnJykq6srq2apBdXevhxQPeMZ8P3vw/nnF3UIU1NTeJ7Hrl27wt3S5wQdHfCYx7jLSvG4yzjecw/85jcugP7hD+Gzn10+pqsLzjvPPTTnn+8+P+OM0HvoShXSU0ikUh07Bk99qqv+/d733L/lRZTrdjYtLS0bHreZwnXfd1NDP/853Hmn+/iLX7jVa+BWmJ11lkvinHee2wXmnHNcYBWWyclJgiAoTKf1zdi+3QXaj3scXHqpqxJfa64tZOl0mpGREZqamtizZ09R7vNEjY3LAdNKk5Mu0LrrruVg+1//1fUIAzeDfs458KhHucsjH+mmE7WdoeRCgZVIJZqZcfUzw8MuO/GoRxV9CNkGVtZa0ul0KCsCrXU1Nnfc4S633+4Cqfl5d3tDgwucXvhC95Ccd557Yyx0yZnneTQ2NpZX48veXhdcPf7xLgD/8Y/dPGeBDQ8PEwQBg4ODZdc5v7PTPRwrZ0fTaTeV+ItfuMvPfw6f+Qx85CPu9ro6F2zt3w8XXOAuZ52lzJasTU8NkUozPw9XXumKhb72NXjCE0oyjGwDq3Q6jbU26x5WtbW1vz/n+LjbCu9nP3NB1O23u7oacMHSeee5Vl3797tA6swzi/+GF4/HmZ+fL8+9704/Hb71LXjiE11w9YMfuB4PBXL06FFmZmbo6+urmAavtbXLU8EvfrG7LgjcAss771y+fPaz8G//5m5vanLZrAsugEc/2k1FDgyoZkscBVYilWRmxtXN/OQn7pX+iitKNpRsi9ezbbUwPw8//GGc3/ymgQcfdMHU2BiL9+GyBs973nLW4Oyzy2O1l+d5GGNKW7S+nnPPhW98w00JPvGJ8J3vuI6cIYvH44yNjdHe3s727dtDP38xRSJuNeG+fa4lHLhg6/77lzOlt9/uslrvf7+7fft2F2Q9+tFw4YXu4zrN92ULU2AlUikmJ9303513wuc+PSLz6QAAG7dJREFU56KMEspkMtTU1Gw43bNaYBUEbjHjLbfArbfCbbe5AmPfTwDbOOUUVx70mMe4N6jzzgunaWXYgiBgYmKCjo6OrPdMLImLL3ZTxpdf7jKc3/2uS7GEZKm1Qk1NDQMhnrecRCLLfbNe9CJ3XTrtarZuu205s3rjjcsrEs880wVZj3mM+/jwh2sKsRroVyxSCTzPTeXccw9cdx0861mlHlHWzUGTySRzc/CDH9Rx220ukLr11uWO221tLnh605uS7NwZcNlljcWuw9+0qakpMplM+RStr+fii1226tJLXZHRd7/rUjIhOHToEPF4nNNOO628A8yQ1dYuF8m/6lXuupkZl9Vaep5/4xvw6U+725qbXbb1sY91gdaFF4a7iELKQ/X8BYhUqsOHXUuFBx90NVWXXVbqEQHLGasTWeuKgW+5xV1+8IMU999fC0Qwxk3hPec5y28sZ53lsgFTU3EefBB6e8t3K5sTeZ5HXV0dbZWyz8oFF7gVpE99qstcfec77heSh5mZGY4cOcL27dtpb28PaaCVq60N/uAP3AXc38Pw8HJ29pZb4H3vg8USRU45BS66yAVbj32sawFSRbHplqRfn0g5Gxtzrb4PHnT/+i69WpeBpX0C5+ZcvclPf7ocTE1MuGPa2+ERj0jxtKfVceWVLjO11nvv0orASil6TiaTzM7OlqylwKade67rbfWUp7iaq29/2821bkImk2F4eJiGhgb2FqBuayswxu17ODi4vMvQwoJbfbj093LzzW4lIris1qMfvRxsXXih25RbKocCK5Fy9etfu0L1yUlXH3PxxaUe0XH/fd9wQ4a77qrnvvtcLylw2adnP3v5v+8zz4R77knS1NTEKaesf+54PE59fX1eXbqLKRaLAdBdie96Z5/tVgg++ckuuLruOpfFytHIyAiZTIbTTjut9I1RK0hTk6shfNzj3Ncr/65uucX9k/Ke9yz/XZ15pvt7uugidznzzKJsBSqbpMBKpBzddBM8//muDfj3v+/WdpdAIuH+s17KRv30p8ub4jY2ZnjkI6O85S3L/1mvthNNKpWiI4vlUYlEomKyVdZaPM+jvb295JtFb9q+fa631dOf7oraP/IReOUrs/52z/OYmppi7969NJXjyoIKslpWa37eZYKX/u6+9rXlvRE7Otzf21JW6zGP2dyOAVIYCqxEys2HPgSvfa3rL3DDDQVZGr+WgweXX8hvucUtQEyn3W2nnuqSGhddBBdeaEmlfHp7o+zevfb5su1hZa0lkUhUTI3OzMwM6XS6MorW19PX51p3PP/5cM01rp/Ae9+7YTokmUwyOjpKa2srO3fuLNJgq0tzMzzpSe4CLqt1//3HZ7X+5m/c9ZGIW3G4lNV67GPdJuHqq1UaCqxEyoXvwxveAB/4ADzzmfBf/+V2oC2QZNJ1ml56ob71Vhgddbc1NLg657/4i+VpvZWrlzIZn1/9KrweVolEAmtt3nsEFovneUSj0YoJBNfV1uYC+Ne9Dv7P/3Gt7T/zGffOvgprLUNDQxhjtmxrhXJkjNuV6Iwz4Oqr3XXT08sF8bfc4rqwfPSj7rZt21xWa+nv94ILCvpyIisosBIpB7Ozbg7g61+H17/eLRsKsdbIWlcHv/QifOutbopvaY+0/n5XwrX0Inzuuevvj5Zt1/Xk4h1sFFhtZo/AUkmn00xPT7Njx46y27Jl06JRt2ne6ae7598TnwjXXw+rFOaPj48zPz/PKaecUrnToFtEe7vrnnHppe5r34ff/nY5o3Xrre4lBVxW65xzjm/1sG+fslqFoMBKpNR+8Qs3FfPQQ/DhD8OrX533Kefn3TTeUvPNW2+FQ4fcbQ0NbguYP/uz5UAq14Vt2QZWSxmrjaYC4/E4xpiKqLGKxWJYayt/GvBExris1amnunbj550H//mfy+/awNzcHOPj43R3d9O5WkGdlFRNzfL2PEvlchMT7jVgKau1ch/Erq7lIGupGa+6xedPgZVIqVjrsgRvfKPbD+O7393Uvn++D/fe67o+33bbyi7m7vZTTnF1Gkv/qZ5zzvrZqGzkElhFo9ENV4wtrQishAyQ53m0tLRURBC4Kc94hnsyPf/5rmfaX/4l/O3f4kciDA0NUV9fX577Isqqurrc2oTLL3dfL71eLGWub73VrZVZ6hZ/xhkuyFq6POIR+b9eVBsFViKlMDEBL385fPWr7o3sU59yRREbsNbVQS3tVXbbba7L89ycu7293f3X+Za3LP8HWoht23LZJzCb6aJEIlERK8vm5uZIJpPsXq9ifys4+2wXXP2v/wXXXgs//CEHrr2WdHMzZ5xxRsW0xJCT1dTAwx7mLq94hbtuetq9jiz9Y/bNb8J//Ie7rb7edZZf2nD6ggvcFKLaPaxNgZVIsf30p3DVVTA+Dv/0T+7Na41MzZEjy5u+Ln08etTdVlvrZmte+tLlIKpYL3i51FhtlNkJgoBkMlkR/aA8z6OmpqY6psEaG10l9JOfzMQrXsHEFVew+/3vp7lErT+kcNrbXUuz/9/euQfJWVUJ/He+7plMJpNJJiEkkjckAWMIEAhI4WoURCOURtAyllqoWyCuPFYpd6O41K4WoO4CamBVBPERCgyYZaOFy8PHlhYPgbCACSCGJDAJyWQyeTKPfp394/SXr6fn1YGe/ma6z6/q1L399Z3uc7/pvn3uueeee/bZ9lgVXnnFjKwnnjAb+/bbYfXqqP2pp1pIwdKlVs6e7fFaIW5YOU6lOHgQrrkGvvc9OwD3kUdsRMqza5fFRT31lBlRTz1l6Q+Aw0fBfOAD0WB20kk2m4yDTCaDiJTksRpq59xoCVzPZrPs3buXyZMn11QyzNSKFbzS2Mi4Vat4y8UX25L1jTfCtGlxq+YMEyJmKM2eHZ31nslYYHzoJX/ySbjppigdy1FHmbG1ZImVp55au8aWG1aOUwnWr7do8dZW9HOX8uo/XM+GzRPYsN525z39dBRcLmKbs971LjOiwsFqJG2VLuUA5kwmQy6Xq5odgR0dHeRyueoLWh+EMLWCTp/O3EcfRW68Ea67zoJyvvUtW0uqISOzlkkmLd7qxBOjJcTubovnDL3pGzbYynEY3zlpko1d4UHVS5bUxjKiG1aOM4yktmyn8+IrmPjbdbw2eRHXnvIL7lp7Jh0/sOeDwI6Bec97bOA57TQrR3oW5YEOYC6k1BxWXV1dBEEw5M7BuGlvb2fs2LGjIhasXOzatYtDhw4xZ84cxowfbxkpV66ESy+Fz33OAnF++EML2HFqjjDf3dKl0Wbm7m549lnzuG/YYOV3vhN5tsaNM2/7ySdH5aJFdsxPteCGleOUAVU76uXZZ02ef7qbt/3hFi5+7d+oJ8M/803+89CXWJis48ILo9nbiSeOzgElPIB5MErNYTUajrLp7Oyks7OzpnbDdXZ2smPHDlpaWnrHvx1/vC0H/vSnltD2lFMs99WqVf2faeTUFA0NFu95+unRtVTKlhGffjqSNWssuwzYBHP+fDO0Fi+OPGOzZ49O75YbVo5zhOzdCxs3wl/+YhLW29shSZrPcAfXBl/nLbntvHDscl664mY+875juW5+WXN+xkomkxnSw3QkOayam5vLpttw0N7ejoiMigD7cpDL5diyZQvJZJJZs2b1bSBi6b/POw++/GVLaHvrrVa/4oqRtW7txE59vRlNJ50UZY0PD55+5plInngC1q6N/q6pybxZJ54Y7WRctAimTh3ZsVtuWDlOP6jC7t02y9q0qXcZxkKBLdktWgQf/lCOC9N3886Hr2Hsjs1wxplw3RpOWLaME+LrxrCRyWQYN8CRJyGpVIpEIjHokmEmkyGdTo/o+KpcLkdHRwctLS01k2agtbWV7u5uFixYMLhncsoU+MlP7Oyjr30Nrr7ajmS6+mpbKhzhy7tOfBQePL1iRXT94EGbrD73XCTr1sGPfhS1mTTJjKyFCy2UIpQZM0aGweWGlVPTpFKW8PzFFy1pXmHZ0RG1a2qyL+4555ghFc6cZh7dg9yz1mbszz1nU7Jbf23b90bCN3yYKCXGqqenpyoC1/ft20c2m62ZoPV9+/axe/dupk6dyvhSg/0WL7YNGo8+aknUrrwSbrjBkt9edJGdR+g4JTB+fJQNPkTV0syEqwMbN5qsXWsrCCFNTXDCCdGZiqHMn1/ZkAs3rJyqJ502l/PmzXa+7F//Ci+9ZOXWrZDLRW2nTbMv5kc/amU4I+ozE9qxA37wAwvcbWuzxnfdZXuTR2NQwBGQy+VQ1ZKyrleDYdXe3s6YMWNKNzJGMel0mm3bttHY2Mj06dOP/AXOPNPir377Wwt0v+IK8159+tO2K3bBgrLr7FQ/Irb8N3WqbfQJCQ2u55+P5IUX4I9/hDvv7P0aM2eagRXKvHlWHnusxYWVEzesnFGPqnmXtmyJ5OWXzZDavNkS3YXbf8FmNQsW2E6WT3zCvlzhzGbQlEuqlnvqllvgnnvsRc87z348zjmnqj1UhRzJcTZDGSNdXV0kEgnq6urKpl856enp4eDBg2/MyBiFbN26lVwux9y5c9/48UIi9n045xwLmlm92iYhq1fbuSqXXQbnnmv79x3nTVBocC1b1vu5zk6bQL/4okk4ob7nnt6rEQDTp9sRmccdZ4bWccfZEuWcOW8snss/2c6IJ5ezWcm2bWYkbdvWW7ZssXX5QiZPti/H299uxlP4pZk//wi/KKoWVXnXXfCLX9gbNjfD5ZfDF75gL1pjlGJYZbNZstlsSTsCR7q3CqiJoPW2tjYOHDjArFmzyrdLc+lSS8nw7W9bcPv3v2+TkSlTzC28ciWcdVbVe3mdytPYGAXMF9PRYasXL73UexL+wAO9Y2jBvFlz5piUihtWTqyk05amYPt2kx07oLXV5NVXTbZvj3KghEyYYFtxZ82yRJphEOSxx9oX4E2FdORyljPhvvvg7rttupNMwnvfC9/4Bnz4wzW966kUw6rUVAtdXV1MmjSpfMqVEVVlz549TJgwYcR61MpFV1cXra2tTJgwgSnDcbjktGl26sCqVXD//fa9uuMO228/fbod+HzBBXY2k3uynGFm0qS+KSFCOjstRGTrVpu0h+WWLaW/vn+CnbKjah6kXbsi2bnTjsYrlra26FT1kLo6i2maOdMmszNnRhIeszDEKSlHzo4d8NBDkbS1mVtr2TLb8XTBBSUdklwLlHIAcympFlKpFNlsdsR6rPbv3086na76oPUwu3oikWD27NnD+2b19bYFbMUKOzn8V78yI2v1ajsmp7nZgmjOPdekBj3CTrw0Nlps7cKFfZ8rdaXDDStnSLJZ23mxZ49Je7uVu3f3lbY2M6S6u/u+ThDYMty0aXDMMXZUyzHH2IS1UI46aphXBjIZy53w5z+bPPKIbTEBOPpo80yFA7ufhxaxeTPccAOZn/0MXn+dZFMTfOpTliQy/AHMt0nl29QP0qbr5z+HQ4cY21+bGPvHmjVw6BDtjY3UnXceE669FiZOjE+vclLUR5qa2L5iBV0XXsi8Zcsq65lrarLDyD/+cdi3Dx5+GB580OS++6zN3LnwjndE7oU4D8h0nBIRLXYXVIjTTjtNn3zyyVjeuxbp7oYDB2D//kj27TPZuzeqd3T0lX37+nqVQhoaLFzi6KOtnDIlCiacOtWuh8bUlCkxJMjcsydKQrVpU3TGQmenPd/SYgP22WebQbV4scd79MdvfgMf+Qik07yWTrMDWAJIXZ25GO+919rl27Sm0+wGTgF7vp82O9NptgMnA4nCNsuXx9o/0mnSwLPAtGSS6fX18elVTor6CHAAeCmZZEoyyax160ZGH1UtAObBB83Yeuwxc3mDfUZOPtnOfgrdCm99qw0wNbJ5xIkPEXlKVU8bsl0phpWIvB/4LpAAblPVbxY9Pwb4GXAqsAf4mKpuHew13bDqH1XLrdTZafL66yaF9ddft6W2Q4ciOXgwkgMHetf377fXHIwgsEn5pEl9paXFvEiTJ0fl5MlmKI0bF/N4lk5bQFZxRPvLL5sh1dYWtR071gynM84wY+qMM8xD4gPy4GzebPctb4y+CrSTN5pCxo61D2/eVbkZ6AZ6nSDX0GD3Op9iYSv2w764sE1jo8W3VdJzVdQ/gJ3AdmARMCYuvcpJP33MAJuwQf2tQDBS+6hqgZaPPx55mTdssMEtZOJEM7LmzYviBUKZObP8++mdmqRUw2rIpUARSQC3AO8FWoEnRGS9qm4qaPb3wF5VnSciK4FvAR97Y6pXlmzWfpszmWgiF0oq1bcslJ6eqBxIurtNwnpXV28Jr3V2RmVhXqVSaGoyA6e52ZKrNTfbWBLWJ0wwCevNzWYsTZwYSVNTzPZFLmcWY2gl7t0budLCent7FLAVBm/t2dPXnTZtmi0hnH9+NKNduNAi3d0bdeTccEOv3QNZ+hk4itZ+U+QNkkLyAe0hXUCf6Kp0Gm66CW6++Y1qe+QU9Q/McBxPQR/i0Kuc9NPHbZhxNQ8IYOT2UcSCLmfMgAsvtGuqFqQZHocQeqV//3szwooH0ZaW3q70qVNtZtjS0lsmTrQBMhxUPZDeeQMM6bESkTOBf1XV9+UffwVAVa8vaPNAvs2jIpLEJnxTdJAXPyr5Fj2/+bOAoBqgKoCQ/4oXXAvybaweldF1JbB2Gr5WcLhNcV0JQANUE9HfDQs5AkkjkiEIMoiYJAK7JkE6qkuaIDAprAeSJghSBEEKkRRBkCaRfxwEKRJBDxKkEZTQJiq2jUT7PhdeG+hxkH9cXCZU+9STqiSKJKlKMpcjqUpdvqzP5XrJmHzZkM0yNptlbAnWZFcQ0FFfz976evbW1bG3vp6O+nraxoxhV0MDO8eMYXdDAyk3nsrLn/7UKxFYF5ADBjvQ5hBmfA3kJ9B8m3r6McASCYurqRRF/ctgfWwAekUcVVqvclLUxzTmURyD/Q8OM5r7mCeRyzEllWJqd7dJTw+TUilaUikr02laUimaCpPbDUBPENCVSNAdBPQkEvQEAakgOFymgoCsCOkgICNCJl/PivQrOSAnYlJQ13xdse+GFtUJ6wWPB7oWXi8s6adNMUcaFBRPEFG8XL9xY3k8VsB0zPsf0gqcMVAbVc2IyH5gMjbxO4yIXAJcArZm+JO915Xw9qOY8FN/hB6o0UYWeg0ehweZICBdUIYD0cFk8nC9K5HoVw4mkxxKJq2sq+NQ/m+cGCj6AVL6GvDFzyuDT1nCQbnfNiX84JWVovfLYP3rMzhWWq9yUqC7Aj3YEmCfUPXR3Mc82SBgZ0MDO4dY/kvmcjRlMozPZHqVY8PJXjZLYybD2FyOsdls74lhNsu4/LXiSWRdLtd7slmhfjvDz/VDNwEqvCtQVW8FbgU4beFCZc2aSr79yOFI1tz6a1t4rb96f2VxfSAJgkgKHycSURnWk0lIJEgEgQ8e1Uxzc68MrD3Yj/Ng3qhubHAZaI9Z2KaOfgah5mb4wx/euL5HSlH/cpjHqo9HrtJ6lZOiPnZjRm2fLGOjuY8jFVUzWDMZK7NZW6osrKtaWSiqA0v4usWP+ysHqw92bag+1SKnnlpSs1IMq+3AzILHM/LX+mvTml8KnIAFsQ9MYyMsWVKSko7jxMgnPwm33XY4Rqffze6h4Z5fKu43M1UpberqLPVCJSnqX0A/RlUcepWToj72axSP9j6OVERsEurxWjVDKWsrTwDzRWSuiNQDK4H1RW3WAxfl6x8BfjdYfJXjOKOIq66yH93BaGgYOr/QmDFD786qq4MvfvHI9HuzlNK/OPQqJ7XQR8cZIQxpWKlqBrgMeAB4HlirqhtF5Osi8sF8s9uBySLyN+BLwKrhUthxnApz3HGWx6mxse+Pc12dXf/lL2HdusHbrFtn7QZrc++9ld/uX0r/4tCrnNRCHx1nhFBSNLCq3q+qC1T1OFW9Nn/tGlVdn693q+pHVXWeqp6uqi8Pp9KO41SY5cstx9Ell1gcThBYeckldn358vK1Gan9G+3UQh8dZwTgmdcdx3Ecx3GGoNQEob5/3XEcx3Ecp0y4YeU4juM4jlMm3LByHMdxHMcpE25YOY7jOI7jlAk3rBzHcRzHccqEG1aO4ziO4zhlIrZ0CyJyEHgxljevXY6i6GBsZ9jxe155/J5XHr/nlcfveeU5XlXHD9UozsOLXiwlH4RTPkTkSb/nlcXveeXxe155/J5XHr/nlUdESkq+6UuBjuM4juM4ZcINK8dxHMdxnDIRp2F1a4zvXav4Pa88fs8rj9/zyuP3vPL4Pa88Jd3z2ILXHcdxHMdxqg1fCnQcx3EcxykTsRtWInK5iLwgIhtF5Ntx61MriMhVIqIiclTculQ7IvLv+c/4syLyXyIyMW6dqhUReb+IvCgifxORVXHrU+2IyEwR+b2IbMqP4VfGrVMtICIJEXlaRH4dty61gohMFJF782P58yJy5kBtYzWsROTdwIeAk1T1bcB/xKlPrSAiM4FzgVfi1qVGeAhYpKqLgb8CX4lZn6pERBLALcByYCHwcRFZGK9WVU8GuEpVFwJvB77g97wiXAk8H7cSNcZ3gf9R1ROAkxjk/sftsfo88E1V7QFQ1baY9akVbgL+CfAAuwqgqg+qaib/8DFgRpz6VDGnA39T1ZdVNQXcjU3cnGFCVV9T1Q35+kHsx2Z6vFpVNyIyAzgPuC1uXWoFEZkAvBO4HUBVU6q6b6D2cRtWC4C/E5HHReR/RWRpzPpUPSLyIWC7qj4Tty41ymeB38StRJUyHXi14HEr/iNfMURkDnAK8Hi8mlQ938Emxrm4Fakh5gK7gTvyS7C3ici4gRoPe+Z1EXkYmNbPU1fn338S5kJeCqwVkWPVtyq+KYa451/FlgGdMjLYPVfV/863uRpbOrmzkro5znAjIk3AL4F/VNUDcetTrYjI+UCbqj4lIsvi1qeGSAJLgMtV9XER+S6wCviXgRoPK6p6zkDPicjngXV5Q+rPIpLDzj/aPdx6VTMD3XMRORGzvJ8REbAlqQ0icrqq7qygilXHYJ9zABH5NHA+cLZPHIaN7cDMgscz8tecYURE6jCj6k5VXRe3PlXOWcAHReQDQAPQLCJrVPWTMetV7bQCraoaemPvxQyrfol7KfA+4N0AIrIAqMcPlRw2VPU5VT1aVeeo6hzsw7LEjarhRUTej7nuP6iqnXHrU8U8AcwXkbkiUg+sBNbHrFNVIzZDux14XlVvjFufakdVv6KqM/Lj90rgd25UDT/538hXReT4/KWzgU0DtY/zEGaAHwM/FpG/ACngIp/NO1XIzcAY4KG8p/AxVb00XpWqD1XNiMhlwANAAvixqm6MWa1q5yzgU8BzIvJ/+WtfVdX7Y9TJcYaDy4E785O2l4HPDNTQM687juM4juOUibiXAh3HcRzHcaoGN6wcx3Ecx3HKhBtWjuM4juM4ZcINK8dxHMdxnDLhhpXjOI7jOE6ZcMPKcRzHcRynTLhh5TiO4ziOUybcsHIcx3EcxykT/w+q2iDFgKfuGQAAAABJRU5ErkJggg==\n",
         "text/plain": "<Figure size 720x360 with 1 Axes>"
        },
        "metadata": {
         "needs_background": "light"
        },
        "output_type": "display_data"
       }
      ]
     }
    },
    "c4a1ac6edc18412c9e497c0a28d14451": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
